{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bbwl6E1E205R",
        "outputId": "3f71af51-c0a1-4996-e1a3-5a0d172d98bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers\n",
        "# T5Tokenizer requires the SentencePiece library\n",
        "%pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wB441x104K-o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QG3Q0E-jKHOZ"
      },
      "outputs": [],
      "source": [
        "# define logging\n",
        "import logging\n",
        "LOG_FILE = f\"./outputs/t5_finetuning_{np.datetime64('now')}\"\n",
        "logging.basicConfig(filename=LOG_FILE, filemode=\"w\", encoding='utf-8', level=logging.DEBUG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8y6kzPpOI27H"
      },
      "outputs": [],
      "source": [
        "# import modules from huggingface\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tlYaKW9h4ai_"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2Gn4rTgENb8"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "y6nEben93JAk"
      },
      "outputs": [],
      "source": [
        "INPUT_DIR = \"./data/\"\n",
        "train_df = pd.read_csv(INPUT_DIR+\"qg_train.csv\", index_col=0)[:10000]\n",
        "val_df = pd.read_csv(INPUT_DIR+\"qg_dev.csv\", index_col=0)[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "AYfBicZQ59Jf"
      },
      "outputs": [],
      "source": [
        "# for question generation task, prepend answer to sentence\n",
        "train_df[\"input\"] = \"answer: \"+train_df[\"answer\"]+\" context: \"+train_df[\"context\"]\n",
        "val_df[\"input\"] = \"answer: \"+val_df[\"answer\"]+\" context: \"+val_df[\"context\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "81f4PKa1F6aM",
        "outputId": "c680be7a-9b80-4062-d18a-ccab456683e5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7c7088e6-f485-46fc-83c7-287c0c14e9ca\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>sentence</th>\n",
              "      <th>answer</th>\n",
              "      <th>context</th>\n",
              "      <th>exact</th>\n",
              "      <th>input</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>True</td>\n",
              "      <td>answer: in the late 1990s context: Beyoncé Gis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>True</td>\n",
              "      <td>answer: singing and dancing context: Beyoncé G...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>Their hiatus saw the release of Beyoncé's debu...</td>\n",
              "      <td>2003</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>True</td>\n",
              "      <td>answer: 2003 context: Beyoncé Giselle Knowles-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>In what city and state did Beyonce  grow up?</td>\n",
              "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
              "      <td>Houston, Texas</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>True</td>\n",
              "      <td>answer: Houston, Texas context: Beyoncé Gisell...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In which decade did Beyonce become famous?</td>\n",
              "      <td>Born and raised in Houston, Texas, she perform...</td>\n",
              "      <td>late 1990s</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>True</td>\n",
              "      <td>answer: late 1990s context: Beyoncé Giselle Kn...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c7088e6-f485-46fc-83c7-287c0c14e9ca')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7c7088e6-f485-46fc-83c7-287c0c14e9ca button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7c7088e6-f485-46fc-83c7-287c0c14e9ca');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0           When did Beyonce start becoming popular?   \n",
              "1  What areas did Beyonce compete in when she was...   \n",
              "2  When did Beyonce leave Destiny's Child and bec...   \n",
              "3      In what city and state did Beyonce  grow up?    \n",
              "4         In which decade did Beyonce become famous?   \n",
              "\n",
              "                                            sentence               answer  \\\n",
              "0  Born and raised in Houston, Texas, she perform...    in the late 1990s   \n",
              "1  Born and raised in Houston, Texas, she perform...  singing and dancing   \n",
              "2  Their hiatus saw the release of Beyoncé's debu...                 2003   \n",
              "3  Born and raised in Houston, Texas, she perform...       Houston, Texas   \n",
              "4  Born and raised in Houston, Texas, she perform...           late 1990s   \n",
              "\n",
              "                                             context  exact  \\\n",
              "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   True   \n",
              "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   True   \n",
              "2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   True   \n",
              "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   True   \n",
              "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   True   \n",
              "\n",
              "                                               input  \n",
              "0  answer: in the late 1990s context: Beyoncé Gis...  \n",
              "1  answer: singing and dancing context: Beyoncé G...  \n",
              "2  answer: 2003 context: Beyoncé Giselle Knowles-...  \n",
              "3  answer: Houston, Texas context: Beyoncé Gisell...  \n",
              "4  answer: late 1990s context: Beyoncé Giselle Kn...  "
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsfoZbjqENcC",
        "outputId": "e52d9169-3de7-49a4-8eee-960f2d38d816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training (11976, 2) samples\n",
            "Test (500, 2) samples\n"
          ]
        }
      ],
      "source": [
        "logging.info(f\"[Data]: Reading data...\\n\")\n",
        "\n",
        "source_text = \"input\"\n",
        "target_text = \"question\"\n",
        "\n",
        "# reset index because CSV index is a mess\n",
        "train_dataset = train_df.reset_index()[[source_text,target_text]]\n",
        "val_dataset = val_df.reset_index()[[source_text,target_text]]\n",
        "\n",
        "print(f\"Training {train_dataset.shape} samples\")\n",
        "print(f\"Test {val_dataset.shape} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho-HJA-6ENcD"
      },
      "source": [
        "### Define dataset class and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "8vLQPGAn4v17"
      },
      "outputs": [],
      "source": [
        "class QuestionSentenceDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Creating a custom dataset \n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, data, tokenizer, source_len, target_len, source_text, target_text):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = data\n",
        "    self.source_len = source_len\n",
        "    self.out_len = target_len\n",
        "    self.target_text = self.data[target_text]\n",
        "    self.source_text = self.data[source_text]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.target_text)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    source_text = str(self.source_text[index])\n",
        "    target_text = str(self.target_text[index])\n",
        "    \n",
        "    source_text = ' '.join(source_text.split())\n",
        "    target_text = ' '.join(target_text.split())\n",
        "\n",
        "    # from text to ids\n",
        "    source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "    target = self.tokenizer.batch_encode_plus([target_text], max_length= self.out_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "\n",
        "    source_ids = source['input_ids'].squeeze()\n",
        "    source_mask = source['attention_mask'].squeeze()\n",
        "    target_ids = target['input_ids'].squeeze()\n",
        "    target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "    return {\n",
        "        'source_ids': source_ids.to(dtype=torch.long), \n",
        "        'source_mask': source_mask.to(dtype=torch.long), \n",
        "        'target_ids': target_ids.to(dtype=torch.long),\n",
        "        'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Nkj6wIMt40RK"
      },
      "outputs": [],
      "source": [
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "\n",
        "  \"\"\"\n",
        "  Function for training\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  model.train()\n",
        "  for _, data in enumerate(loader):\n",
        "    y = data['target_ids'].to(device, dtype = torch.long)\n",
        "    y_ids = y[:, :-1].contiguous()\n",
        "    lm_labels = y[:, 1:].clone().detach()\n",
        "    # padding of labels is done with a token with id -100\n",
        "    # which is a special token automatically ignored by PyTorch loss functions\n",
        "    lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100 \n",
        "    ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "    mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
        "    loss = outputs[0]\n",
        "\n",
        "    if _%10==0:\n",
        "      print(f\"Epoch {epoch}, Step {_}, Loss = {loss}\")\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "GUBykK-A43DF"
      },
      "outputs": [],
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "\n",
        "  \"\"\"\n",
        "  Function to get predictions from model\n",
        "\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  with torch.no_grad():\n",
        "      for _, data in enumerate(loader):\n",
        "          y = data['target_ids'].to(device, dtype = torch.long)\n",
        "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "          generated_ids = model.generate(\n",
        "              input_ids = ids,\n",
        "              attention_mask = mask, \n",
        "              max_length=150, \n",
        "              num_beams=2,\n",
        "              repetition_penalty=2.5, \n",
        "              length_penalty=1.0, \n",
        "              early_stopping=True\n",
        "              )\n",
        "          \n",
        "          # get words from ids\n",
        "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "          if _%10==0:\n",
        "              print(f'Completed {_}')\n",
        "\n",
        "          predictions.extend(preds)\n",
        "          actuals.extend(target)\n",
        "\n",
        "  return predictions, actuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Tw4RW_qO4_8T"
      },
      "outputs": [],
      "source": [
        "def T5Trainer(train_dataset, val_dataset, source_text, target_text, model_params, output_dir=\"./outputs/\" ):\n",
        "  \n",
        "  \"\"\"\n",
        "  T5 train and validate\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # to be able to reproduce\n",
        "  torch.manual_seed(model_params[\"SEED\"])\n",
        "  np.random.seed(model_params[\"SEED\"])\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  logging.info(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
        "\n",
        "  # encode text\n",
        "  tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
        "\n",
        "  # using T5 with language model layer\n",
        "  model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
        "  model = model.to(device)\n",
        "  \n",
        "  # create dataloaders\n",
        "  train_qsd = QuestionSentenceDataset(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
        "  val_qsd = QuestionSentenceDataset(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
        "\n",
        "  train_params = {\n",
        "      'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "      'shuffle': True,\n",
        "      'num_workers': 0\n",
        "      }\n",
        "\n",
        "\n",
        "  val_params = {\n",
        "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
        "      'shuffle': False,\n",
        "      'num_workers': 0\n",
        "      }\n",
        "\n",
        "  train_loader = DataLoader(train_qsd, **train_params)\n",
        "  val_loader = DataLoader(val_qsd, **val_params)\n",
        "\n",
        "  # training loop\n",
        "  optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
        "\n",
        "  logging.info(f'[Initiating Fine Tuning]...\\n')\n",
        "\n",
        "  for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "      train(epoch, tokenizer, model, device, train_loader, optimizer)\n",
        "      \n",
        "      # checkpoint\n",
        "      if epoch%10:\n",
        "        cp_path = os.path.join(output_dir, f\"checkpoint{epoch}\")\n",
        "        model.save_pretrained(cp_path)\n",
        "      \n",
        "  logging.info(f\"[Saving Model]...\\n\")\n",
        "  # save model, tokenizer and configs\n",
        "  path = os.path.join(output_dir, \"final\")\n",
        "  model.save_pretrained(path)\n",
        "  tokenizer.save_pretrained(path)\n",
        "\n",
        "\n",
        "  # evaluating test dataset\n",
        "  logging.info(f\"[Initiating Validation]...\\n\")\n",
        "  for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
        "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "    final_df = pd.DataFrame({'generated':predictions,'actual':actuals})\n",
        "    final_df.to_csv(os.path.join(output_dir,'predictions.csv'))\n",
        "  \n",
        "  logging.info(f\"[Validation Completed.]\\n\")\n",
        "  print(f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"checkpoints\")}\\n\"\"\")\n",
        "  print(f\"\"\"[Validation] Generated questions saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\")\n",
        "  print(f\"\"\"[Logs] Logs saved @ {LOG_FILE}\\n\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "PxCpQwD8PDIs"
      },
      "outputs": [],
      "source": [
        "model_params={\n",
        "    \"MODEL\":\"t5-small\",            # pretrained model\n",
        "    \"TRAIN_BATCH_SIZE\":8,          # training batch size\n",
        "    \"VALID_BATCH_SIZE\":8,          # validation batch size\n",
        "    \"TRAIN_EPOCHS\":3,              # number of training epochs\n",
        "    \"VAL_EPOCHS\":1,                # number of validation epochs\n",
        "    \"LEARNING_RATE\":1e-3,          # learning rate\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\":64,   # max length of source text\n",
        "    \"MAX_TARGET_TEXT_LENGTH\":15,   # max length of target text\n",
        "    \"SEED\": 42                     # set seed for reproducibility \n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qijZoYeI55fM",
        "outputId": "a1de3e92-e77c-4371-cb3f-d99570e5ed85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5.py:174: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Step 0, Loss = 5.642310619354248\n",
            "Epoch 0, Step 10, Loss = 3.9199302196502686\n",
            "Epoch 0, Step 20, Loss = 3.993957996368408\n",
            "Epoch 0, Step 30, Loss = 4.337299823760986\n",
            "Epoch 0, Step 40, Loss = 3.613666534423828\n",
            "Epoch 0, Step 50, Loss = 3.673809289932251\n",
            "Epoch 0, Step 60, Loss = 3.2799713611602783\n",
            "Epoch 0, Step 70, Loss = 3.6796414852142334\n",
            "Epoch 0, Step 80, Loss = 2.8729088306427\n",
            "Epoch 0, Step 90, Loss = 3.1814584732055664\n",
            "Epoch 0, Step 100, Loss = 3.1381988525390625\n",
            "Epoch 0, Step 110, Loss = 3.5035743713378906\n",
            "Epoch 0, Step 120, Loss = 2.7879228591918945\n",
            "Epoch 0, Step 130, Loss = 3.412121057510376\n",
            "Epoch 0, Step 140, Loss = 3.274005889892578\n",
            "Epoch 0, Step 150, Loss = 2.9916586875915527\n",
            "Epoch 0, Step 160, Loss = 2.828334093093872\n",
            "Epoch 0, Step 170, Loss = 3.5482983589172363\n",
            "Epoch 0, Step 180, Loss = 3.1731836795806885\n",
            "Epoch 0, Step 190, Loss = 2.6266489028930664\n",
            "Epoch 0, Step 200, Loss = 3.124974250793457\n",
            "Epoch 0, Step 210, Loss = 3.5272293090820312\n",
            "Epoch 0, Step 220, Loss = 2.5071282386779785\n",
            "Epoch 0, Step 230, Loss = 2.4576621055603027\n",
            "Epoch 0, Step 240, Loss = 3.0933847427368164\n",
            "Epoch 0, Step 250, Loss = 3.3114545345306396\n",
            "Epoch 0, Step 260, Loss = 3.7781689167022705\n",
            "Epoch 0, Step 270, Loss = 3.1308212280273438\n",
            "Epoch 0, Step 280, Loss = 3.3708901405334473\n",
            "Epoch 0, Step 290, Loss = 2.9276106357574463\n",
            "Epoch 0, Step 300, Loss = 2.8169071674346924\n",
            "Epoch 0, Step 310, Loss = 3.3611395359039307\n",
            "Epoch 0, Step 320, Loss = 2.841278314590454\n",
            "Epoch 0, Step 330, Loss = 3.5171329975128174\n",
            "Epoch 0, Step 340, Loss = 3.7213170528411865\n",
            "Epoch 0, Step 350, Loss = 3.2060348987579346\n",
            "Epoch 0, Step 360, Loss = 2.829198122024536\n",
            "Epoch 0, Step 370, Loss = 2.748095989227295\n",
            "Epoch 0, Step 380, Loss = 2.6120285987854004\n",
            "Epoch 0, Step 390, Loss = 2.343658685684204\n",
            "Epoch 0, Step 400, Loss = 3.2969233989715576\n",
            "Epoch 0, Step 410, Loss = 2.6672286987304688\n",
            "Epoch 0, Step 420, Loss = 3.033620834350586\n",
            "Epoch 0, Step 430, Loss = 1.8433432579040527\n",
            "Epoch 0, Step 440, Loss = 3.142669439315796\n",
            "Epoch 0, Step 450, Loss = 3.1571402549743652\n",
            "Epoch 0, Step 460, Loss = 2.8959648609161377\n",
            "Epoch 0, Step 470, Loss = 1.9110745191574097\n",
            "Epoch 0, Step 480, Loss = 2.6534247398376465\n",
            "Epoch 0, Step 490, Loss = 3.1838228702545166\n",
            "Epoch 0, Step 500, Loss = 2.4964022636413574\n",
            "Epoch 0, Step 510, Loss = 2.8749771118164062\n",
            "Epoch 0, Step 520, Loss = 2.5728514194488525\n",
            "Epoch 0, Step 530, Loss = 3.0856118202209473\n",
            "Epoch 0, Step 540, Loss = 2.1709749698638916\n",
            "Epoch 0, Step 550, Loss = 3.199218988418579\n",
            "Epoch 0, Step 560, Loss = 2.1179540157318115\n",
            "Epoch 0, Step 570, Loss = 2.2567999362945557\n",
            "Epoch 0, Step 580, Loss = 2.4449708461761475\n",
            "Epoch 0, Step 590, Loss = 2.909580945968628\n",
            "Epoch 0, Step 600, Loss = 3.06270694732666\n",
            "Epoch 0, Step 610, Loss = 2.622371196746826\n",
            "Epoch 0, Step 620, Loss = 2.7128615379333496\n",
            "Epoch 0, Step 630, Loss = 2.5904531478881836\n",
            "Epoch 0, Step 640, Loss = 3.0694403648376465\n",
            "Epoch 0, Step 650, Loss = 2.6997156143188477\n",
            "Epoch 0, Step 660, Loss = 3.780113697052002\n",
            "Epoch 0, Step 670, Loss = 3.33896803855896\n",
            "Epoch 0, Step 680, Loss = 3.04453182220459\n",
            "Epoch 0, Step 690, Loss = 2.2899959087371826\n",
            "Epoch 0, Step 700, Loss = 3.0368876457214355\n",
            "Epoch 0, Step 710, Loss = 2.5107975006103516\n",
            "Epoch 0, Step 720, Loss = 3.534281015396118\n",
            "Epoch 0, Step 730, Loss = 3.289111375808716\n",
            "Epoch 0, Step 740, Loss = 2.7513599395751953\n",
            "Epoch 0, Step 750, Loss = 2.9601142406463623\n",
            "Epoch 0, Step 760, Loss = 2.574748992919922\n",
            "Epoch 0, Step 770, Loss = 3.397463798522949\n",
            "Epoch 0, Step 780, Loss = 2.379638433456421\n",
            "Epoch 0, Step 790, Loss = 2.5353689193725586\n",
            "Epoch 0, Step 800, Loss = 2.47607421875\n",
            "Epoch 0, Step 810, Loss = 3.252617835998535\n",
            "Epoch 0, Step 820, Loss = 2.288719415664673\n",
            "Epoch 0, Step 830, Loss = 3.535930871963501\n",
            "Epoch 0, Step 840, Loss = 2.891010046005249\n",
            "Epoch 0, Step 850, Loss = 3.9262382984161377\n",
            "Epoch 0, Step 860, Loss = 2.424373149871826\n",
            "Epoch 0, Step 870, Loss = 3.6676578521728516\n",
            "Epoch 0, Step 880, Loss = 2.7128262519836426\n",
            "Epoch 0, Step 890, Loss = 2.7892863750457764\n",
            "Epoch 0, Step 900, Loss = 2.304651975631714\n",
            "Epoch 0, Step 910, Loss = 3.0218656063079834\n",
            "Epoch 0, Step 920, Loss = 3.114795446395874\n",
            "Epoch 0, Step 930, Loss = 2.582974672317505\n",
            "Epoch 0, Step 940, Loss = 3.2354955673217773\n",
            "Epoch 0, Step 950, Loss = 2.412961959838867\n",
            "Epoch 0, Step 960, Loss = 2.2222952842712402\n",
            "Epoch 0, Step 970, Loss = 3.2529971599578857\n",
            "Epoch 0, Step 980, Loss = 2.5995302200317383\n",
            "Epoch 0, Step 990, Loss = 2.7645857334136963\n",
            "Epoch 0, Step 1000, Loss = 2.1587212085723877\n",
            "Epoch 0, Step 1010, Loss = 1.9528000354766846\n",
            "Epoch 0, Step 1020, Loss = 2.8205645084381104\n",
            "Epoch 0, Step 1030, Loss = 2.4047324657440186\n",
            "Epoch 0, Step 1040, Loss = 2.951789140701294\n",
            "Epoch 0, Step 1050, Loss = 2.2598516941070557\n",
            "Epoch 0, Step 1060, Loss = 2.324651002883911\n",
            "Epoch 0, Step 1070, Loss = 2.755528211593628\n",
            "Epoch 0, Step 1080, Loss = 2.870908737182617\n",
            "Epoch 0, Step 1090, Loss = 2.415818214416504\n",
            "Epoch 0, Step 1100, Loss = 2.227471113204956\n",
            "Epoch 0, Step 1110, Loss = 2.557851552963257\n",
            "Epoch 0, Step 1120, Loss = 2.5906178951263428\n",
            "Epoch 0, Step 1130, Loss = 2.328127384185791\n",
            "Epoch 0, Step 1140, Loss = 2.312880277633667\n",
            "Epoch 0, Step 1150, Loss = 2.9941442012786865\n",
            "Epoch 0, Step 1160, Loss = 2.9197962284088135\n",
            "Epoch 0, Step 1170, Loss = 2.7903122901916504\n",
            "Epoch 0, Step 1180, Loss = 2.775642156600952\n",
            "Epoch 0, Step 1190, Loss = 2.5024609565734863\n",
            "Epoch 0, Step 1200, Loss = 2.769716739654541\n",
            "Epoch 0, Step 1210, Loss = 2.171135425567627\n",
            "Epoch 0, Step 1220, Loss = 2.824082612991333\n",
            "Epoch 0, Step 1230, Loss = 2.657731294631958\n",
            "Epoch 0, Step 1240, Loss = 1.7561546564102173\n",
            "Epoch 0, Step 1250, Loss = 2.4770538806915283\n",
            "Epoch 0, Step 1260, Loss = 2.7395009994506836\n",
            "Epoch 0, Step 1270, Loss = 2.3708271980285645\n",
            "Epoch 0, Step 1280, Loss = 3.243675947189331\n",
            "Epoch 0, Step 1290, Loss = 2.6904091835021973\n",
            "Epoch 0, Step 1300, Loss = 2.7282443046569824\n",
            "Epoch 0, Step 1310, Loss = 3.991560935974121\n",
            "Epoch 0, Step 1320, Loss = 2.22194766998291\n",
            "Epoch 0, Step 1330, Loss = 2.9384942054748535\n",
            "Epoch 0, Step 1340, Loss = 2.966911792755127\n",
            "Epoch 0, Step 1350, Loss = 3.2006418704986572\n",
            "Epoch 0, Step 1360, Loss = 3.0727362632751465\n",
            "Epoch 0, Step 1370, Loss = 2.3380658626556396\n",
            "Epoch 0, Step 1380, Loss = 2.6070525646209717\n",
            "Epoch 0, Step 1390, Loss = 2.1145167350769043\n",
            "Epoch 0, Step 1400, Loss = 2.673781394958496\n",
            "Epoch 0, Step 1410, Loss = 2.5141100883483887\n",
            "Epoch 0, Step 1420, Loss = 3.325669765472412\n",
            "Epoch 0, Step 1430, Loss = 2.335965871810913\n",
            "Epoch 0, Step 1440, Loss = 2.9153571128845215\n",
            "Epoch 0, Step 1450, Loss = 3.3838350772857666\n",
            "Epoch 0, Step 1460, Loss = 2.915355920791626\n",
            "Epoch 0, Step 1470, Loss = 2.745699167251587\n",
            "Epoch 0, Step 1480, Loss = 3.010028600692749\n",
            "Epoch 0, Step 1490, Loss = 3.1193063259124756\n",
            "Epoch 1, Step 0, Loss = 2.0752196311950684\n",
            "Epoch 1, Step 10, Loss = 2.260749340057373\n",
            "Epoch 1, Step 20, Loss = 1.7722480297088623\n",
            "Epoch 1, Step 30, Loss = 2.42185115814209\n",
            "Epoch 1, Step 40, Loss = 2.3748340606689453\n",
            "Epoch 1, Step 50, Loss = 1.9471195936203003\n",
            "Epoch 1, Step 60, Loss = 1.772265076637268\n",
            "Epoch 1, Step 70, Loss = 1.6305768489837646\n",
            "Epoch 1, Step 80, Loss = 1.7306746244430542\n",
            "Epoch 1, Step 90, Loss = 2.449280023574829\n",
            "Epoch 1, Step 100, Loss = 1.4086077213287354\n",
            "Epoch 1, Step 110, Loss = 2.7369801998138428\n",
            "Epoch 1, Step 120, Loss = 2.1514687538146973\n",
            "Epoch 1, Step 130, Loss = 2.5197994709014893\n",
            "Epoch 1, Step 140, Loss = 2.36967134475708\n",
            "Epoch 1, Step 150, Loss = 2.3591158390045166\n",
            "Epoch 1, Step 160, Loss = 2.580291986465454\n",
            "Epoch 1, Step 170, Loss = 3.0394327640533447\n",
            "Epoch 1, Step 180, Loss = 1.7723044157028198\n",
            "Epoch 1, Step 190, Loss = 1.7305554151535034\n",
            "Epoch 1, Step 200, Loss = 2.623295545578003\n",
            "Epoch 1, Step 210, Loss = 1.9436967372894287\n",
            "Epoch 1, Step 220, Loss = 2.6091456413269043\n",
            "Epoch 1, Step 230, Loss = 1.848283052444458\n",
            "Epoch 1, Step 240, Loss = 1.603297472000122\n",
            "Epoch 1, Step 250, Loss = 2.237738609313965\n",
            "Epoch 1, Step 260, Loss = 1.4180465936660767\n",
            "Epoch 1, Step 270, Loss = 2.0092415809631348\n",
            "Epoch 1, Step 280, Loss = 2.5017647743225098\n",
            "Epoch 1, Step 290, Loss = 2.4562082290649414\n",
            "Epoch 1, Step 300, Loss = 3.001513957977295\n",
            "Epoch 1, Step 310, Loss = 2.5031232833862305\n",
            "Epoch 1, Step 320, Loss = 2.258695125579834\n",
            "Epoch 1, Step 330, Loss = 2.3621320724487305\n",
            "Epoch 1, Step 340, Loss = 2.4265620708465576\n",
            "Epoch 1, Step 350, Loss = 1.9045846462249756\n",
            "Epoch 1, Step 360, Loss = 2.300865888595581\n",
            "Epoch 1, Step 370, Loss = 2.297534227371216\n",
            "Epoch 1, Step 380, Loss = 2.723729133605957\n",
            "Epoch 1, Step 390, Loss = 2.207529067993164\n",
            "Epoch 1, Step 400, Loss = 2.008345127105713\n",
            "Epoch 1, Step 410, Loss = 1.9723389148712158\n",
            "Epoch 1, Step 420, Loss = 2.4031670093536377\n",
            "Epoch 1, Step 430, Loss = 2.518312692642212\n",
            "Epoch 1, Step 440, Loss = 2.3932673931121826\n",
            "Epoch 1, Step 450, Loss = 2.3274648189544678\n",
            "Epoch 1, Step 460, Loss = 2.759593963623047\n",
            "Epoch 1, Step 470, Loss = 2.752000093460083\n",
            "Epoch 1, Step 480, Loss = 3.1788713932037354\n",
            "Epoch 1, Step 490, Loss = 1.9996830224990845\n",
            "Epoch 1, Step 500, Loss = 3.049415111541748\n",
            "Epoch 1, Step 510, Loss = 1.506438136100769\n",
            "Epoch 1, Step 520, Loss = 2.861851692199707\n",
            "Epoch 1, Step 530, Loss = 2.286757707595825\n",
            "Epoch 1, Step 540, Loss = 2.0787062644958496\n",
            "Epoch 1, Step 550, Loss = 2.1807916164398193\n",
            "Epoch 1, Step 560, Loss = 2.143170118331909\n",
            "Epoch 1, Step 570, Loss = 2.557016134262085\n",
            "Epoch 1, Step 580, Loss = 2.081393003463745\n",
            "Epoch 1, Step 590, Loss = 2.512723445892334\n",
            "Epoch 1, Step 600, Loss = 1.588253378868103\n",
            "Epoch 1, Step 610, Loss = 2.2226176261901855\n",
            "Epoch 1, Step 620, Loss = 1.5097062587738037\n",
            "Epoch 1, Step 630, Loss = 2.4142041206359863\n",
            "Epoch 1, Step 640, Loss = 2.0401194095611572\n",
            "Epoch 1, Step 650, Loss = 2.6665871143341064\n",
            "Epoch 1, Step 660, Loss = 2.5019309520721436\n",
            "Epoch 1, Step 670, Loss = 2.0856337547302246\n",
            "Epoch 1, Step 680, Loss = 2.670315742492676\n",
            "Epoch 1, Step 690, Loss = 1.7097232341766357\n",
            "Epoch 1, Step 700, Loss = 2.22758412361145\n",
            "Epoch 1, Step 710, Loss = 2.5449419021606445\n",
            "Epoch 1, Step 720, Loss = 1.985725998878479\n",
            "Epoch 1, Step 730, Loss = 2.3833377361297607\n",
            "Epoch 1, Step 740, Loss = 2.224046230316162\n",
            "Epoch 1, Step 750, Loss = 1.673405408859253\n",
            "Epoch 1, Step 760, Loss = 2.0891847610473633\n",
            "Epoch 1, Step 770, Loss = 1.91519296169281\n",
            "Epoch 1, Step 780, Loss = 2.508305311203003\n",
            "Epoch 1, Step 790, Loss = 2.3159873485565186\n",
            "Epoch 1, Step 800, Loss = 2.0479633808135986\n",
            "Epoch 1, Step 810, Loss = 2.201035737991333\n",
            "Epoch 1, Step 820, Loss = 2.706407070159912\n",
            "Epoch 1, Step 830, Loss = 2.769038438796997\n",
            "Epoch 1, Step 840, Loss = 1.9450488090515137\n",
            "Epoch 1, Step 850, Loss = 2.9972753524780273\n",
            "Epoch 1, Step 860, Loss = 2.1672275066375732\n",
            "Epoch 1, Step 870, Loss = 1.9263216257095337\n",
            "Epoch 1, Step 880, Loss = 2.2907164096832275\n",
            "Epoch 1, Step 890, Loss = 1.7368824481964111\n",
            "Epoch 1, Step 900, Loss = 3.0657873153686523\n",
            "Epoch 1, Step 910, Loss = 2.5534656047821045\n",
            "Epoch 1, Step 920, Loss = 2.84407639503479\n",
            "Epoch 1, Step 930, Loss = 2.7867164611816406\n",
            "Epoch 1, Step 940, Loss = 2.5762412548065186\n",
            "Epoch 1, Step 950, Loss = 2.9698574542999268\n",
            "Epoch 1, Step 960, Loss = 2.0836238861083984\n",
            "Epoch 1, Step 970, Loss = 2.2394182682037354\n",
            "Epoch 1, Step 980, Loss = 2.1584200859069824\n",
            "Epoch 1, Step 990, Loss = 2.1618528366088867\n",
            "Epoch 1, Step 1000, Loss = 2.671689748764038\n",
            "Epoch 1, Step 1010, Loss = 1.818062424659729\n",
            "Epoch 1, Step 1020, Loss = 2.2950315475463867\n",
            "Epoch 1, Step 1030, Loss = 2.030261754989624\n",
            "Epoch 1, Step 1040, Loss = 2.4591028690338135\n",
            "Epoch 1, Step 1050, Loss = 2.8777761459350586\n",
            "Epoch 1, Step 1060, Loss = 1.645185112953186\n",
            "Epoch 1, Step 1070, Loss = 1.4518202543258667\n",
            "Epoch 1, Step 1080, Loss = 2.665156126022339\n",
            "Epoch 1, Step 1090, Loss = 2.359264612197876\n",
            "Epoch 1, Step 1100, Loss = 1.834343671798706\n",
            "Epoch 1, Step 1110, Loss = 2.562633752822876\n",
            "Epoch 1, Step 1120, Loss = 2.0751187801361084\n",
            "Epoch 1, Step 1130, Loss = 1.7545404434204102\n",
            "Epoch 1, Step 1140, Loss = 2.072413444519043\n",
            "Epoch 1, Step 1150, Loss = 2.5665910243988037\n",
            "Epoch 1, Step 1160, Loss = 2.3958160877227783\n",
            "Epoch 1, Step 1170, Loss = 1.9272644519805908\n",
            "Epoch 1, Step 1180, Loss = 1.794523000717163\n",
            "Epoch 1, Step 1190, Loss = 2.2749624252319336\n",
            "Epoch 1, Step 1200, Loss = 2.1436305046081543\n",
            "Epoch 1, Step 1210, Loss = 2.5944595336914062\n",
            "Epoch 1, Step 1220, Loss = 2.113607168197632\n",
            "Epoch 1, Step 1230, Loss = 2.3734729290008545\n",
            "Epoch 1, Step 1240, Loss = 2.4243881702423096\n",
            "Epoch 1, Step 1250, Loss = 1.8218380212783813\n",
            "Epoch 1, Step 1260, Loss = 2.1106348037719727\n",
            "Epoch 1, Step 1270, Loss = 2.243717670440674\n",
            "Epoch 1, Step 1280, Loss = 2.617462396621704\n",
            "Epoch 1, Step 1290, Loss = 2.2719109058380127\n",
            "Epoch 1, Step 1300, Loss = 2.6194214820861816\n",
            "Epoch 1, Step 1310, Loss = 2.5712831020355225\n",
            "Epoch 1, Step 1320, Loss = 2.2788443565368652\n",
            "Epoch 1, Step 1330, Loss = 2.2744946479797363\n",
            "Epoch 1, Step 1340, Loss = 2.4423398971557617\n",
            "Epoch 1, Step 1350, Loss = 2.3740832805633545\n",
            "Epoch 1, Step 1360, Loss = 1.7649588584899902\n",
            "Epoch 1, Step 1370, Loss = 1.9652316570281982\n",
            "Epoch 1, Step 1380, Loss = 2.3633806705474854\n",
            "Epoch 1, Step 1390, Loss = 2.544721841812134\n",
            "Epoch 1, Step 1400, Loss = 2.7013866901397705\n",
            "Epoch 1, Step 1410, Loss = 2.458775758743286\n",
            "Epoch 1, Step 1420, Loss = 2.233353614807129\n",
            "Epoch 1, Step 1430, Loss = 1.9430338144302368\n",
            "Epoch 1, Step 1440, Loss = 2.392573118209839\n",
            "Epoch 1, Step 1450, Loss = 1.7769849300384521\n",
            "Epoch 1, Step 1460, Loss = 2.2269835472106934\n",
            "Epoch 1, Step 1470, Loss = 2.3831284046173096\n",
            "Epoch 1, Step 1480, Loss = 2.0830559730529785\n",
            "Epoch 1, Step 1490, Loss = 2.465507984161377\n",
            "Epoch 2, Step 0, Loss = 1.5731992721557617\n",
            "Epoch 2, Step 10, Loss = 2.0040266513824463\n",
            "Epoch 2, Step 20, Loss = 1.522234559059143\n",
            "Epoch 2, Step 30, Loss = 1.747833251953125\n",
            "Epoch 2, Step 40, Loss = 2.0857162475585938\n",
            "Epoch 2, Step 50, Loss = 2.3253285884857178\n",
            "Epoch 2, Step 60, Loss = 1.5861320495605469\n",
            "Epoch 2, Step 70, Loss = 2.2028331756591797\n",
            "Epoch 2, Step 80, Loss = 2.175337314605713\n",
            "Epoch 2, Step 90, Loss = 1.620850920677185\n",
            "Epoch 2, Step 100, Loss = 1.8657121658325195\n",
            "Epoch 2, Step 110, Loss = 2.3758339881896973\n",
            "Epoch 2, Step 120, Loss = 2.1405274868011475\n",
            "Epoch 2, Step 130, Loss = 2.455303430557251\n",
            "Epoch 2, Step 140, Loss = 1.9751341342926025\n",
            "Epoch 2, Step 150, Loss = 2.3506765365600586\n",
            "Epoch 2, Step 160, Loss = 1.7556079626083374\n",
            "Epoch 2, Step 170, Loss = 2.245213270187378\n",
            "Epoch 2, Step 180, Loss = 2.3607075214385986\n",
            "Epoch 2, Step 190, Loss = 1.400499939918518\n",
            "Epoch 2, Step 200, Loss = 2.0794622898101807\n",
            "Epoch 2, Step 210, Loss = 1.9683860540390015\n",
            "Epoch 2, Step 220, Loss = 1.580914855003357\n",
            "Epoch 2, Step 230, Loss = 1.8107777833938599\n",
            "Epoch 2, Step 240, Loss = 2.3741276264190674\n",
            "Epoch 2, Step 250, Loss = 1.899867296218872\n",
            "Epoch 2, Step 260, Loss = 1.832319974899292\n",
            "Epoch 2, Step 270, Loss = 1.7216005325317383\n",
            "Epoch 2, Step 280, Loss = 1.3134217262268066\n",
            "Epoch 2, Step 290, Loss = 2.0884671211242676\n",
            "Epoch 2, Step 300, Loss = 1.644066333770752\n",
            "Epoch 2, Step 310, Loss = 1.688455581665039\n",
            "Epoch 2, Step 320, Loss = 1.9342316389083862\n",
            "Epoch 2, Step 330, Loss = 1.8304921388626099\n",
            "Epoch 2, Step 340, Loss = 1.612239956855774\n",
            "Epoch 2, Step 350, Loss = 1.709423542022705\n",
            "Epoch 2, Step 360, Loss = 2.2725391387939453\n",
            "Epoch 2, Step 370, Loss = 2.545356035232544\n",
            "Epoch 2, Step 380, Loss = 2.500735282897949\n",
            "Epoch 2, Step 390, Loss = 2.1070170402526855\n",
            "Epoch 2, Step 400, Loss = 2.150169849395752\n",
            "Epoch 2, Step 410, Loss = 2.366975784301758\n",
            "Epoch 2, Step 420, Loss = 1.729832410812378\n",
            "Epoch 2, Step 430, Loss = 1.5503625869750977\n",
            "Epoch 2, Step 440, Loss = 1.9879481792449951\n",
            "Epoch 2, Step 450, Loss = 2.079857349395752\n",
            "Epoch 2, Step 460, Loss = 2.2036731243133545\n",
            "Epoch 2, Step 470, Loss = 2.42156982421875\n",
            "Epoch 2, Step 480, Loss = 1.819661021232605\n",
            "Epoch 2, Step 490, Loss = 2.026822805404663\n",
            "Epoch 2, Step 500, Loss = 2.4125094413757324\n",
            "Epoch 2, Step 510, Loss = 2.1170036792755127\n",
            "Epoch 2, Step 520, Loss = 2.1492607593536377\n",
            "Epoch 2, Step 530, Loss = 1.4567879438400269\n",
            "Epoch 2, Step 540, Loss = 1.790589690208435\n",
            "Epoch 2, Step 550, Loss = 3.111004590988159\n",
            "Epoch 2, Step 560, Loss = 2.3651859760284424\n",
            "Epoch 2, Step 570, Loss = 2.1114304065704346\n",
            "Epoch 2, Step 580, Loss = 1.6693311929702759\n",
            "Epoch 2, Step 590, Loss = 1.500648856163025\n",
            "Epoch 2, Step 600, Loss = 1.564719319343567\n",
            "Epoch 2, Step 610, Loss = 2.422492027282715\n",
            "Epoch 2, Step 620, Loss = 2.3389439582824707\n",
            "Epoch 2, Step 630, Loss = 2.6545169353485107\n",
            "Epoch 2, Step 640, Loss = 1.9314805269241333\n",
            "Epoch 2, Step 650, Loss = 2.2057902812957764\n",
            "Epoch 2, Step 660, Loss = 2.2024128437042236\n",
            "Epoch 2, Step 670, Loss = 1.4450101852416992\n",
            "Epoch 2, Step 680, Loss = 1.5753135681152344\n",
            "Epoch 2, Step 690, Loss = 1.6232285499572754\n",
            "Epoch 2, Step 700, Loss = 2.1309401988983154\n",
            "Epoch 2, Step 710, Loss = 2.3691186904907227\n",
            "Epoch 2, Step 720, Loss = 2.245009422302246\n",
            "Epoch 2, Step 730, Loss = 2.0964467525482178\n",
            "Epoch 2, Step 740, Loss = 2.4145431518554688\n",
            "Epoch 2, Step 750, Loss = 1.8632348775863647\n",
            "Epoch 2, Step 760, Loss = 1.7693920135498047\n",
            "Epoch 2, Step 770, Loss = 1.5496610403060913\n",
            "Epoch 2, Step 780, Loss = 1.9878637790679932\n",
            "Epoch 2, Step 790, Loss = 2.0165481567382812\n",
            "Epoch 2, Step 800, Loss = 1.340928077697754\n",
            "Epoch 2, Step 810, Loss = 1.606253743171692\n",
            "Epoch 2, Step 820, Loss = 1.560175895690918\n",
            "Epoch 2, Step 830, Loss = 1.9346377849578857\n",
            "Epoch 2, Step 840, Loss = 1.4249120950698853\n",
            "Epoch 2, Step 850, Loss = 2.231447696685791\n",
            "Epoch 2, Step 860, Loss = 2.0460641384124756\n",
            "Epoch 2, Step 870, Loss = 1.827513575553894\n",
            "Epoch 2, Step 880, Loss = 1.6884702444076538\n",
            "Epoch 2, Step 890, Loss = 1.3494017124176025\n",
            "Epoch 2, Step 900, Loss = 1.8939436674118042\n",
            "Epoch 2, Step 910, Loss = 1.9556527137756348\n",
            "Epoch 2, Step 920, Loss = 1.73981773853302\n",
            "Epoch 2, Step 930, Loss = 2.0089855194091797\n",
            "Epoch 2, Step 940, Loss = 2.26554274559021\n",
            "Epoch 2, Step 950, Loss = 1.8806017637252808\n",
            "Epoch 2, Step 960, Loss = 1.7911903858184814\n",
            "Epoch 2, Step 970, Loss = 2.194417953491211\n",
            "Epoch 2, Step 980, Loss = 1.279500126838684\n",
            "Epoch 2, Step 990, Loss = 1.7288144826889038\n",
            "Epoch 2, Step 1000, Loss = 2.1257998943328857\n",
            "Epoch 2, Step 1010, Loss = 2.0198075771331787\n",
            "Epoch 2, Step 1020, Loss = 2.256051540374756\n",
            "Epoch 2, Step 1030, Loss = 1.4132169485092163\n",
            "Epoch 2, Step 1040, Loss = 2.346377372741699\n",
            "Epoch 2, Step 1050, Loss = 2.0915725231170654\n",
            "Epoch 2, Step 1060, Loss = 2.0933074951171875\n",
            "Epoch 2, Step 1070, Loss = 2.651155710220337\n",
            "Epoch 2, Step 1080, Loss = 1.7612130641937256\n",
            "Epoch 2, Step 1090, Loss = 1.7180923223495483\n",
            "Epoch 2, Step 1100, Loss = 1.73921799659729\n",
            "Epoch 2, Step 1110, Loss = 1.7441672086715698\n",
            "Epoch 2, Step 1120, Loss = 1.7802711725234985\n",
            "Epoch 2, Step 1130, Loss = 1.677648901939392\n",
            "Epoch 2, Step 1140, Loss = 2.256617307662964\n",
            "Epoch 2, Step 1150, Loss = 2.1338977813720703\n",
            "Epoch 2, Step 1160, Loss = 1.6053729057312012\n",
            "Epoch 2, Step 1170, Loss = 2.0046582221984863\n",
            "Epoch 2, Step 1180, Loss = 1.872559666633606\n",
            "Epoch 2, Step 1190, Loss = 1.515092372894287\n",
            "Epoch 2, Step 1200, Loss = 2.067894220352173\n",
            "Epoch 2, Step 1210, Loss = 2.020808696746826\n",
            "Epoch 2, Step 1220, Loss = 2.2812602519989014\n",
            "Epoch 2, Step 1230, Loss = 2.363220691680908\n",
            "Epoch 2, Step 1240, Loss = 1.7350413799285889\n",
            "Epoch 2, Step 1250, Loss = 1.9014614820480347\n",
            "Epoch 2, Step 1260, Loss = 2.5274925231933594\n",
            "Epoch 2, Step 1270, Loss = 1.648598313331604\n",
            "Epoch 2, Step 1280, Loss = 1.5362893342971802\n",
            "Epoch 2, Step 1290, Loss = 2.4611427783966064\n",
            "Epoch 2, Step 1300, Loss = 1.873991847038269\n",
            "Epoch 2, Step 1310, Loss = 2.061497688293457\n",
            "Epoch 2, Step 1320, Loss = 1.692336916923523\n",
            "Epoch 2, Step 1330, Loss = 2.6198298931121826\n",
            "Epoch 2, Step 1340, Loss = 1.8558809757232666\n",
            "Epoch 2, Step 1350, Loss = 1.8548471927642822\n",
            "Epoch 2, Step 1360, Loss = 2.0886073112487793\n",
            "Epoch 2, Step 1370, Loss = 2.154053211212158\n",
            "Epoch 2, Step 1380, Loss = 1.696964144706726\n",
            "Epoch 2, Step 1390, Loss = 2.0653700828552246\n",
            "Epoch 2, Step 1400, Loss = 1.961207389831543\n",
            "Epoch 2, Step 1410, Loss = 1.3242381811141968\n",
            "Epoch 2, Step 1420, Loss = 1.7948111295700073\n",
            "Epoch 2, Step 1430, Loss = 1.7026411294937134\n",
            "Epoch 2, Step 1440, Loss = 1.8780972957611084\n",
            "Epoch 2, Step 1450, Loss = 1.867012858390808\n",
            "Epoch 2, Step 1460, Loss = 2.3595829010009766\n",
            "Epoch 2, Step 1470, Loss = 1.7803694009780884\n",
            "Epoch 2, Step 1480, Loss = 2.043480396270752\n",
            "Epoch 2, Step 1490, Loss = 1.638109803199768\n",
            "Completed 0\n",
            "Completed 10\n",
            "Completed 20\n",
            "Completed 30\n",
            "Completed 40\n",
            "Completed 50\n",
            "Completed 60\n",
            "[Model] Model saved @ ./outputs/checkpoints\n",
            "\n",
            "[Validation] Generated questions saved @ ./outputs/predictions.csv\n",
            "\n",
            "[Logs] Logs saved @ ./outputs/t5_finetuning_2022-07-29T13:32:23\n",
            "\n"
          ]
        }
      ],
      "source": [
        "T5Trainer(train_dataset, val_dataset, source_text=\"input\", target_text=\"question\", model_params=model_params, output_dir=\"./outputs/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMRYAMH_aRNz"
      },
      "outputs": [],
      "source": [
        "# TODO: use ROUGE or some metric to evaluate predictions\n",
        "# TODO: check loss function\n",
        "# TODO: do we preprocess/clean input?\n",
        "# TODO: validation being used as test, write validation\n",
        "# TODO: plot loss graphs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "t5_finetuning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
