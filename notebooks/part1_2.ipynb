{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import tokenize\n",
    "df = pd.read_csv('sample_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_text = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    ls_text.append(tokenize.sent_tokenize(df['context'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Transformers method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "This type of symbiosis is relatively uncommon in rudimentary reference texts, but is omnipresent in the natural world.\n",
      "An example of mutual symbiosis is the relationship between the ocellaris clownfish that dwell among the tentacles of Ritteri sea anemones.\n",
      "Another non-obligate symbiosis is known from encrusting bryozoans and hermit crabs that live in a close relationship.\n",
      "Symbiotic relationships include those associations in which one organism lives on another (ectosymbiosis, such as mistletoe), or where one partner lives inside the other (endosymbiosis, such as lactobacilli and other bacteria in humans or Symbiodinium in corals).\n",
      "While historically, symbiosis has received less attention than other interactions such as predation or competition, it is increasingly recognized as an important selective force behind evolution, with many species having a long history of interdependent co-evolution.\n",
      "Many biologists restrict the definition of symbiosis to close mutualist relationships.\n",
      "An example of a biotrophic relationship would be a tick feeding on the blood of its host.\n",
      "Some believe symbiosis should only refer to persistent mutualisms, while others believe it should apply to any type of persistent biological interaction (in other words mutualistic, commensalistic, or parasitic).\n",
      "During mutualistic symbioses, the host cell lacks some of the nutrients, which are provided by the endosymbiont.\n",
      "Symbiosis (from Greek σύν \"together\" and βίωσις \"living\") is close and often long-term interaction between two different biological species.\n",
      "\n",
      "\n",
      "\n",
      "The origins of the Samoans are closely studied in modern research about Polynesia in various scientific disciplines such as genetics, linguistics and anthropology.\n",
      "Rugby union is the national sport in Samoa and the national team, nicknamed the Manu Samoa, is consistently competitive against teams from vastly more populous nations.\n",
      "When Christianity was introduced in Samoa, most Samoan people converted.\n",
      "The Samoan culture is centred around the principle of vāfealoa'i, the relationships between people.\n",
      "Samoa also played in the Pacific Nations Cup and the Pacific Tri-Nations The sport is governed by the Samoa Rugby Football Union, who are members of the Pacific Islands Rugby Alliance, and thus, also contribute to the international Pacific Islanders rugby union team.\n",
      "Samoa also signed a friendship treaty with New Zealand.\n",
      "While all of the islands have volcanic origins, only Savai'i, the western most island in Samoa, is volcanically active with the most recent eruptions in Mt Matavanu (1905–1911), Mata o le Afi (1902) and Mauga Afi (1725).\n",
      "Some Samoans are spiritual and religious, and have subtly adapted the dominant religion of Christianity to 'fit in' with fa'a Samoa and vice versa.\n",
      "Other noteworthy players from NZ and Australia have represented the Samoan National team.\n",
      "The Independent State of Samoa ( Samoan: Malo Sa 'oloto Tuto 'atasi o Sāmoa, IPA: [ˌsaːˈmoa]), commonly known as Samoa (Samoan: Sāmoa) and formerly known as Western Samoa, is a Unitary Parliamentary Republic with eleven administrative divisions.\n",
      "\n",
      "\n",
      "\n",
      "[citation needed] With house music already massive on the '80s dance-scene it was only a matter of time before it would penetrate the UK pop charts.\n",
      "Fingers\", claims that the term \"house\" became popular due to many of the early DJs creating music in their own homes using synthesizers and drum machines such as the Roland TR-808, TR-909, and the TB 303.\n",
      "Key labels in the rise of house music in the UK included: The tour in March 1987[citation needed] of Knuckles, Jefferson, Fingers Inc. (Heard) and Adonis as the DJ International Tour boosted house in the UK.\n",
      "House music proved to be a commercially successful genre and a more mainstream pop-based variation grew increasingly popular.\n",
      "In an effort to maintain such exclusives, the DJs were inspired to create their own \"house\" records.\n",
      "The single is credited as helping to bring house music to the US mainstream.\n",
      "Since the early to mid-1990s, house music has been infused in mainstream pop and dance music worldwide.\n",
      "International record label, doesn't mention Importes Etc., Frankie Knuckles, or the Warehouse by name, but agrees that \"house\" was a regional catch-all term for dance music, and that it was once synonymous with older disco music.\n",
      "2010s saw multiple new sounds in house music developed by numerous DJs.\n",
      "House music is a genre of electronic dance music that originated in Chicago in the early 1980s.\n",
      "\n",
      "\n",
      "\n",
      "Although most Czech vocabulary is based on shared roots with Slavic, Romance, and Germanic languages, many loanwords (most associated with high culture) have been adopted in recent years.\n",
      "By then the language had developed a literary tradition, and since then it has changed little; journals from that period have no substantial differences from modern standard Czech, and contemporary Czechs can understand them with little difficulty.\n",
      "The Czechs' language separated from other Slavic tongues into what would later be called Old Czech by the thirteenth century, a classification extending through the sixteenth century.\n",
      "Czech typographical features not associated with phonetics generally resemble those of most Latin European languages, including English.\n",
      "Czech continued to evolve and gain in regional importance for hundreds of years, and has been a literary language in the Slovak lands since the early fifteenth century.\n",
      "Slovak has slightly more borrowed words than Czech.\n",
      "The consensus among linguists is that modern, standard Czech originated during the eighteenth century.\n",
      "These changes differentiated Czech from Slovak.\n",
      "Since then, \"Czechoslovak\" refers to improvised pidgins of the languages which have arisen from the decrease in mutual intelligibility.\n",
      "Czech has one of the most phonemic orthographies of all European languages.\n",
      "\n",
      "\n",
      "\n",
      "Italians in different regions today may also speak regional varieties of standard Italian, or regional Italian dialects, which, unlike the majority of languages of Italy, are actually dialects of standard Italian rather than separate languages.\n",
      "These regional languages are often referred to colloquially or in non-linguistic circles as Italian \"dialects,\" or dialetti (standard Italian for \"dialects\").\n",
      "In addition to having evolved, for the most part, separately from one another and with distinct individual histories, the Latin-based regional Romance languages of Italy are also better classified as separate languages rather than true \"dialects\" due to the often high degree in which they lack mutual intelligibility.\n",
      "Meanwhile, the \"dialects\" subordinate to the standard language are generally not variations on the standard language but rather separate (but often related) languages in and of themselves.\n",
      "For example, most of the various regional Romance languages of Italy, often colloquially referred to as Italian \"dialects,\" are, in fact, not actually derived from modern standard Italian, but rather evolved from Vulgar Latin separately and individually from one another and independently of standard Italian, long prior to the diffusion of a national standardized language throughout what is now Italy.\n",
      "These various Latin-derived regional languages are therefore, in a linguistic sense, not truly \"dialects\" of the standard Italian language, but are instead better defined as their own separate languages.\n",
      "There may be multiple standard dialects associated with a single language.\n",
      "In this secondary sense of \"dialect\", language varieties are often called dialects rather than languages: The status of \"language\" is not solely determined by linguistic criteria, but it is also the result of a historical and political development.\n",
      "They are therefore better classified as individual languages rather than \"dialects.\"\n",
      "By the definition most commonly used by linguists, any linguistic variety can be considered a \"dialect\" of some language—\"everybody speaks a dialect\".\n",
      "\n",
      "\n",
      "\n",
      "Hanover is of national importance because of its universities and medical school, its international airport and its large zoo.\n",
      "Around 40 theatres are located in Hanover.\n",
      "The Theater für Niedersachsen is another big theatre in Hanover, which also has an own Musical-Company.\n",
      "\"Hanover\" is the traditional English spelling.\n",
      "A large Jewish population then existed in Hanover.\n",
      "Every year Hanover hosts the Schützenfest Hannover, the world's largest marksmen's festival, and the Oktoberfest Hannover, the second largest Oktoberfest in the world (beside Oktoberfest of Blumenau).\n",
      "Hanover was thus a gateway to the Rhine, Ruhr and Saar river valleys, their industrial areas which grew up to the southwest and the plains regions to the east and north, for overland traffic skirting the Harz between the Low Countries and Saxony or Thuringia.\n",
      "But Hanover is not only one of the most important Exhibition Cities in the world, it is also one of the German capitals for marksmen.\n",
      "[citation needed] Some other popular sights are the Waterloo Column, the Laves House, the Wangenheim Palace, the Lower Saxony State Archives, the Hanover Playhouse, the Kröpcke Clock, the Anzeiger Tower Block, the Administration Building of the NORD/LB, the Cupola Hall of the Congress Centre, the Lower Saxony Stock, the Ministry of Finance, the Garten Church, the Luther Church, the Gehry Tower (designed by the American architect Frank O. Gehry), the specially designed Bus Stops, the Opera House, the Central Station, the Maschsee lake and the city forest Eilenriede, which is one of the largest of its kind in Europe.\n",
      "With a population of 518,000, Hanover is a major centre of Northern Germany and the country's thirteenth largest city.\n",
      "\n",
      "\n",
      "\n",
      "Partially digested food fills the duodenum.\n",
      "This allows the mass of food to further mix with the digestive enzymes.\n",
      "In the small intestine, the larger part of digestion takes place and this is helped by the secretions of bile, pancreatic juice and intestinal juice.\n",
      "In chemical digestion, enzymes break down food into the small molecules the body can use.\n",
      "The stomach continues to break food down mechanically and chemically through churning and mixing with both acids and enzymes.\n",
      "It is stimulated by distension of the stomach, presence of food in stomach and decrease in pH.\n",
      "Food is formed into a bolus by the mechanical mastication and swallowed into the esophagus from where it enters the stomach through the action of peristalsis.\n",
      "Digesta is finally moved into the small intestine, where the digestion and absorption of nutrients occurs.\n",
      "In the stomach further release of enzymes break down the food further and this is combined with the churning action of the stomach.\n",
      "Digestion begins in the mouth with the secretion of saliva and its digestive enzymes.\n",
      "\n",
      "\n",
      "\n",
      "The following year, IBM hosted its first Invention Award Dinner honoring 34 outstanding IBM inventors; and in 1963, the company named the first eight IBM Fellows in a new Fellowship Program that recognizes senior IBM scientists, engineers and other professionals for outstanding technical achievements.\n",
      "On September 21, 1953, Thomas Watson, Jr., the company's president at the time, sent out a controversial letter to all IBM employees stating that IBM needed to hire the best people, regardless of their race, ethnic origin, or gender.\n",
      "The company stated that this would give IBM a competitive advantage because IBM would then be able to hire talented people its competitors would turn down.\n",
      "IBM has been a leading proponent of the Open Source Initiative, and began supporting Linux in 1998.\n",
      "IBM's employee management practices can be traced back to its roots.\n",
      "As of 2012[update], IBM had been the top annual recipient of U.S. patents for 20 consecutive years.\n",
      "The initialism IBM followed.\n",
      "IBM acquired Kenexa (2012) and SPSS (2009) and PwC's consulting business (2002), spinning off companies like printer manufacturer Lexmark (1991), and selling off product lines like its personal computer and x86 server businesses to Lenovo (2005, 2014).\n",
      "IBM has constantly evolved since its inception.\n",
      "The program searches for fresh start-up companies that IBM can partner with to solve world problems.\n",
      "\n",
      "\n",
      "\n",
      "The start of Neolithic 1 overlaps the Tahunian and Heavy Neolithic periods to some degree.\n",
      "However, excavations in Central Europe have revealed that early Neolithic Linear Ceramic cultures (\"Linearbandkeramik\") were building large arrangements of circular ditches between 4800 BC and 4600 BC.\n",
      "Not all of these cultural elements characteristic of the Neolithic appeared everywhere in the same order: the earliest farming societies in the Near East did not use pottery.\n",
      "In southeast Europe agrarian societies first appeared in the 7th millennium BC, attested by one of the earliest farming sites of Europe, discovered in Vashtëmi, southeastern Albania and dating back to 6,500 BC.\n",
      "Around 10,200 BC the first fully developed Neolithic cultures belonging to the phase Pre-Pottery Neolithic A (PPNA) appeared in the fertile crescent.\n",
      "The beginning of the Neolithic culture is considered to be in the Levant (Jericho, modern-day West Bank) about 10,200 – 8,800 BC.\n",
      "The shelter of the early people changed dramatically from the paleolithic to the neolithic era.\n",
      "Neolithic people were skilled farmers, manufacturing a range of tools necessary for the tending, harvesting and processing of crops (such as sickle blades and grinding stones) and food production (e.g.\n",
      "The Neolithic is a progression of behavioral and cultural characteristics and changes, including the use of wild and domestic crops and of domesticated animals.\n",
      "Neolithic peoples in the Levant, Anatolia, Syria, northern Mesopotamia and Central Asia were also accomplished builders, utilizing mud-brick to construct houses and villages.\n",
      "\n",
      "\n",
      "\n",
      "According to psychologists, sexual orientation also refers to a person’s choice of sexual partners, who may be homosexual, heterosexual, or bisexual.\n",
      "Some researchers who study sexual orientation argue that the concept may not apply similarly to men and women.\n",
      "Sexual orientation is argued as a concept that evolved in the industrialized West, and there is a controversy as to the universality of its application in other societies or cultures.\n",
      "Research suggests that sexual orientation is independent of cultural and other social influences, but that open identification of one's sexual orientation may be hindered by homophobic/hetereosexist settings.\n",
      "Influences of culture may complicate the process of measuring sexual orientation.\n",
      "Furthermore, there are more than two dimensions of sexuality to be considered.\n",
      "They additionally state that sexual orientation \"is distinct from other components of sex and gender, including biological sex (the anatomical, physiological, and genetic characteristics associated with being male or female), gender identity (the psychological sense of being male or female), and social gender role (the cultural norms that define feminine and masculine behavior)\".\n",
      "Individuals may or may not consider their sexual orientation to define their sexual identity, as they may experience various degrees of fluidity of sexuality, or may simply identify more strongly with another aspect of their identity such as family role.\n",
      "Research over several decades has demonstrated that sexual orientation ranges along a continuum, from exclusive attraction to the opposite sex to exclusive attraction to the same sex.\n",
      "Individuals may or may not express their sexual orientation in their behaviors.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "ans_1 = []\n",
    "scores = []\n",
    "for k in range(len(df)):\n",
    "    print('\\n\\n')\n",
    "    #Compute embedding for both lists\n",
    "    embeddings1 = model.encode(ls_text[k], convert_to_tensor=True)\n",
    "\n",
    "    a = np.matmul(embeddings1,embeddings1.T)\n",
    "    a = np.abs(a).numpy()\n",
    "    a = np.sum(a, axis = 1)\n",
    "    temp = []\n",
    "    temp2 = []\n",
    "    for i in np.argpartition(a, -10)[-10:]:\n",
    "        # print(ls_text[k][i])\n",
    "        temp.append(ls_text[k][i])\n",
    "        print(ls_text[k][i])\n",
    "        temp2.append(a[i])\n",
    "\n",
    "        \n",
    "    # for i in np.argpartition(a, -10)[-10:]:\n",
    "    #     # print(ls_text[k][i])\n",
    "    #     temp.append(ls_text[k][i])\n",
    "    #     # print(ls_text[k][i])\n",
    "    #     print(a[i])\n",
    "\n",
    "    ans_1.append(temp)\n",
    "    scores.append(temp2)\n",
    "\n",
    "\n",
    "    #Compute cosine-similarities\n",
    "    # cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "    # #Output the pairs with their score\n",
    "    # for i in range(len(sentences1)):\n",
    "    #     print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(ans_1).T\n",
    "df1.to_csv('sent_trans_1.csv')\n",
    "df1 = pd.DataFrame(scores).T\n",
    "df1.to_csv('sent_trans_1_scores.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Transformers method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Symbiosis\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Samoa\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: House_music\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Czech_language\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Dialect\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Hanover\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Digestion\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: IBM\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Neolithic\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Sexual_orientation\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "ans_2 = []\n",
    "scores = []\n",
    "for i in range(len(df)):\n",
    "    sents = ls_text[i]\n",
    "    \n",
    "    # Query sentences:\n",
    "    queries = [df['title'][i]]\n",
    "    corpus_embeddings = embedder.encode(sents, convert_to_tensor=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Find the closest 10 sentences of the corpus for each query sentence based on cosine similarity\n",
    "\n",
    "    top_k = min(10, len(sents))\n",
    "    for query in queries:\n",
    "        query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "        # We use cosine-similarity and torch.topk to find the highest 1- scores\n",
    "        cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "        top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "        print(\"\\n\\n======================\\n\\n\")\n",
    "        print(\"Query:\", query)\n",
    "        print(\"\\nTop 10 most similar sentences in corpus:\")\n",
    "        \n",
    "        temp = []\n",
    "        temp2 = []\n",
    "        for score, idx in zip(top_results[0], top_results[1]):\n",
    "            # print(sents[idx], \"(Score: {:.4f})\".format(score))\n",
    "            temp.append(sents[idx])\n",
    "            temp2.append(float(\"{:.4f}\".format(score)))\n",
    "        ans_2.append(temp)\n",
    "        scores.append(temp2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7749, 0.7628, 0.7476, 0.6967, 0.6674, 0.6501, 0.646, 0.6346, 0.6177, 0.6036]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(ans_2).T\n",
    "# df2.to_csv('sent_trans_2.csv')\n",
    "df2 = pd.DataFrame(scores).T\n",
    "df2.to_csv('sent_trans_2_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_frequency_matrix(sentences):\n",
    "    frequency_matrix = {}\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    lemma = WordNetLemmatizer()\n",
    "\n",
    "    for sent in sentences:\n",
    "        freq_table = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = lemma.lemmatize(word)\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        frequency_matrix[sent[:15]] = freq_table\n",
    "\n",
    "    return frequency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_tf_matrix(freq_matrix):\n",
    "    tf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        tf_table = {}\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, count in f_table.items():\n",
    "            tf_table[word] = count / count_words_in_sentence\n",
    "\n",
    "        tf_matrix[sent] = tf_table\n",
    "\n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_documents_per_words(freq_matrix):\n",
    "    word_per_doc_table = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        for word, count in f_table.items():\n",
    "            if word in word_per_doc_table:\n",
    "                word_per_doc_table[word] += 1\n",
    "            else:\n",
    "                word_per_doc_table[word] = 1\n",
    "\n",
    "    return word_per_doc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
    "    idf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        idf_table = {}\n",
    "\n",
    "        for word in f_table.keys():\n",
    "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
    "\n",
    "        idf_matrix[sent] = idf_table\n",
    "\n",
    "    return idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "\n",
    "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "\n",
    "        tf_idf_table = {}\n",
    "\n",
    "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
    "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "\n",
    "        tf_idf_matrix[sent1] = tf_idf_table\n",
    "\n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _score_sentences(tf_idf_matrix) -> dict:\n",
    "    \"\"\"\n",
    "    score a sentence by its word's TF\n",
    "    Basic algorithm: adding the TF frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    sentenceValue = {}\n",
    "\n",
    "    for sent, f_table in tf_idf_matrix.items():\n",
    "        total_score_per_sentence = 0\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, score in f_table.items():\n",
    "            total_score_per_sentence += score\n",
    "\n",
    "        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n",
    "\n",
    "    return sentenceValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_average_score(sentenceValue) -> int:\n",
    "    \"\"\"\n",
    "    Find the average score from the sentence value dictionary\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    sumValues = 0\n",
    "    for entry in sentenceValue:\n",
    "        sumValues += sentenceValue[entry]\n",
    "\n",
    "    # Average value of a sentence from original summary_text\n",
    "    average = (sumValues / len(sentenceValue))\n",
    "\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_summary(sentences, sentenceValue, threshold):\n",
    "    sentence_count = 0\n",
    "    summary = []\n",
    "    sco = []\n",
    "\n",
    "    ls = np.argpartition(list(sentenceValue.values()), -10)[-10:]\n",
    "    threshold = list(sentenceValue.values())[ls[0]]\n",
    "    # ans = []\n",
    "    # for i in ls:\n",
    "    #     ans.append(sentences)\n",
    "    for sentence in sentences:\n",
    "        if sentence[:15] in sentenceValue and sentenceValue[sentence[:15]] >= (threshold):\n",
    "            summary.append(sentence)\n",
    "            sentence_count += 1\n",
    "            sco.append(sentenceValue[sentence[:15]])\n",
    "\n",
    "    return summary,sco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mihir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords    \n",
    "    \n",
    "'''\n",
    "We already have a sentence tokenizer, so we just need \n",
    "to run the sent_tokenize() method to create the array of sentences.\n",
    "'''\n",
    "\n",
    "ans_3 = []\n",
    "scores = []\n",
    "\n",
    "for i in range(len(ls_text)):\n",
    "\n",
    "    sentences = ls_text[i]\n",
    "    # 1 Sentence Tokenize\n",
    "    # sentences = sent_tokenize(text)\n",
    "    total_documents = len(sentences)\n",
    "    #print(sentences)\n",
    "\n",
    "    # 2 Create the Frequency matrix of the words in each sentence.\n",
    "    freq_matrix = _create_frequency_matrix(sentences)\n",
    "    #print(freq_matrix)\n",
    "\n",
    "    '''\n",
    "    Term frequency (TF) is how often a word appears in a document, divided by how many words are there in a document.\n",
    "    '''\n",
    "    # 3 Calculate TermFrequency and generate a matrix\n",
    "    tf_matrix = _create_tf_matrix(freq_matrix)\n",
    "    #print(tf_matrix)\n",
    "\n",
    "    # 4 creating table for documents per words\n",
    "    count_doc_per_words = _create_documents_per_words(freq_matrix)\n",
    "    #print(count_doc_per_words)\n",
    "\n",
    "    '''\n",
    "    Inverse document frequency (IDF) is how unique or rare a word is.\n",
    "    '''\n",
    "    # 5 Calculate IDF and generate a matrix\n",
    "    idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
    "    #print(idf_matrix)\n",
    "\n",
    "    # 6 Calculate TF-IDF and generate a matrix\n",
    "    tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
    "    #print(tf_idf_matrix)\n",
    "\n",
    "    # 7 Important Algorithm: score the sentences\n",
    "    sentence_scores = _score_sentences(tf_idf_matrix)\n",
    "    #print(sentence_scores)\n",
    "\n",
    "    # 8 Find the threshold\n",
    "    threshold = _find_average_score(sentence_scores)\n",
    "    #print(threshold)\n",
    "\n",
    "    # 9 Important Algorithm: Generate the summary\n",
    "    summary,sco = _generate_summary(sentences, sentence_scores, threshold)\n",
    "    # print(summary)\n",
    "    ans_3.append(summary)\n",
    "    scores.append(sco)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(ans_3).T\n",
    "df2.to_csv('tf-idf.csv')\n",
    "df2 = pd.DataFrame(scores).T\n",
    "df2.to_csv('tf-idf_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence_transformer_method1'] = ans_1\n",
    "df['sentence_transformer_method2'] = ans_2\n",
    "df['tf_idf'] = ans_3\n",
    "df.to_csv('sample_text_results_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('adult', 0.3380449104576557), ('theme', 0.21790166957939905), ('themes', 0.21790166957939905), ('united', 0.21790166957939883), ('originally', 0.19383195948940687), ('original seven', 0.19383195948940676), ('bros', 0.1836023327011601), ('harry', 0.18360233270115994), ('film', 0.17862367456062173)]\n"
     ]
    }
   ],
   "source": [
    "TR_keywords = keywords.keywords(summary, scores=True)\n",
    "print(TR_keywords[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keybert\n",
      "  Downloading keybert-0.5.1.tar.gz (19 kB)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from keybert) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from keybert) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from keybert) (1.21.5)\n",
      "Collecting rich>=10.4.0\n",
      "  Downloading rich-12.5.1-py3-none-any.whl (235 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (2.11.2)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.8.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (3.7)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.64.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (1.12.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.1.96)\n",
      "Requirement already satisfied: torchvision in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.13.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (4.1.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.0.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.3.15)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.12.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers>=0.3.8->keybert) (0.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2021.10.8)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (9.0.1)\n",
      "Building wheels for collected packages: keybert\n",
      "  Building wheel for keybert (setup.py): started\n",
      "  Building wheel for keybert (setup.py): finished with status 'done'\n",
      "  Created wheel for keybert: filename=keybert-0.5.1-py3-none-any.whl size=21332 sha256=7f4da0a0827a80cbf122ed41c197f6d28d1deb5a869f7f4cff100b95bf72ddd3\n",
      "  Stored in directory: c:\\users\\mihir\\appdata\\local\\pip\\cache\\wheels\\94\\18\\2a\\f26bbcd25924aab452bb4bcc2345a55c07160823d196a264c7\n",
      "Successfully built keybert\n",
      "Installing collected packages: commonmark, rich, keybert\n",
      "Successfully installed commonmark-0.9.1 keybert-0.5.1 rich-12.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7ea1b1f14c4ce598419fdc21c00add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f875f10a77489ea5ca7ddc6bb36c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c35a5dbb75c4d2faa96768d4240ee60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6d189613984d539f9711bcabb7d269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddfdbf7b70146b995c2bb5e640a0bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31498ee7bdf24a9c966cd8631cee65fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce9b43807ed4e4e80883a5d70321e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5537f88b3a94335a8e70e50c5be3f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860378015f3343928ba832f89ec1f5ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05813adfcce49ffa529cfb8c10b87ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58b5066bc0541b6ba6be0069be3b2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d26c7793a9b477f88db8bb7ab14cbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342da009543047c898a45d5df6c036f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73c1aff70df48feab65f046550e1c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kw_model = KeyBERT(model='all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potter cursed child', 'harry potter cursed', 'rowling main theme', 'potter cursed', 'story written rowling', 'cursed child play', 'harry potter', 'cursed child', 'original seven books', 'written rowling']\n"
     ]
    }
   ],
   "source": [
    "keywords = kw_model.extract_keywords(summary, \n",
    "\n",
    "                                     keyphrase_ngram_range=(1, 3), \n",
    "\n",
    "                                     stop_words='english', \n",
    "\n",
    "                                     highlight=False,\n",
    "\n",
    "                                     top_n=10)\n",
    "\n",
    "keywords_list= list(dict(keywords).keys())\n",
    "\n",
    "print(keywords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cdj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixed types\n",
      "0.18359439311764025 1\n",
      "[mixed types]\n",
      "systems\n",
      "0.1784796193107821 3\n",
      "[systems, systems, systems]\n",
      "minimal generating sets\n",
      "0.15037838042245094 1\n",
      "[minimal generating sets]\n",
      "nonstrict inequations\n",
      "0.14740065982407313 1\n",
      "[nonstrict inequations]\n",
      "strict inequations\n",
      "0.13946027725597837 1\n",
      "[strict inequations]\n",
      "linear Diophantine equations\n",
      "0.1195023546245721 1\n",
      "[linear Diophantine equations]\n",
      "natural numbers\n",
      "0.11450088293222845 1\n",
      "[natural numbers]\n",
      "solutions\n",
      "0.10780718173686318 3\n",
      "[solutions, solutions, solutions]\n",
      "linear constraints\n",
      "0.10529828014583348 1\n",
      "[linear constraints]\n",
      "all the considered types systems\n",
      "0.1036960590708142 1\n",
      "[all the considered types systems]\n",
      "a minimal supporting set\n",
      "0.08812713074893187 1\n",
      "[a minimal supporting set]\n",
      "a system\n",
      "0.08243620500315359 1\n",
      "[a system]\n",
      "a minimal set\n",
      "0.07944607954086784 1\n",
      "[a minimal set]\n",
      "algorithms\n",
      "0.0763527926213032 1\n",
      "[algorithms]\n",
      "all types\n",
      "0.07593126037016427 1\n",
      "[all types]\n",
      "Diophantine\n",
      "0.07309361902551355 1\n",
      "[Diophantine]\n",
      "construction\n",
      "0.0702090100898443 1\n",
      "[construction]\n",
      "Upper bounds\n",
      "0.060225391238828516 1\n",
      "[Upper bounds]\n",
      "the set\n",
      "0.05800111772673988 1\n",
      "[the set]\n",
      "components\n",
      "0.054251394765316464 1\n",
      "[components]\n",
      "Compatibility\n",
      "0.04516904342912139 1\n",
      "[Compatibility]\n",
      "compatibility\n",
      "0.04516904342912139 1\n",
      "[compatibility]\n",
      "the corresponding algorithms\n",
      "0.04435648606848154 1\n",
      "[the corresponding algorithms]\n",
      "Criteria\n",
      "0.042273783712246285 1\n",
      "[Criteria]\n",
      "These criteria\n",
      "0.01952542432474353 1\n",
      "[These criteria]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### Pytextrank to find important phrases\n",
    "\n",
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "# example text\n",
    "text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\"\n",
    "\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# examine the top-ranked phrases in the document\n",
    "for phrase in doc._.phrases:\n",
    "    print(phrase.text)\n",
    "    print(phrase.rank, phrase.count)\n",
    "    print(phrase.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "47c626452ef4ef3e74376d35c302fcf9bdc1b9327d6e04736eb914a557504e89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
