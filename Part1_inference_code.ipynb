{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import torch\n",
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "import pytextrank\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe(\"textrank\")\n",
    "nltk.download('punkt')\n",
    "\n",
    "def semanticsearch(para, topic, k=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes paragraph and it's topic as its input.\n",
    "    Extracts top 5 best sentences best linked to the topic.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    para : string\n",
    "        Text for the pare\n",
    "    topic : string\n",
    "        Text for the topic\n",
    "    k : int\n",
    "        Number of sentences to be selected \n",
    "        (default value is 5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : list of strings\n",
    "        List of k sentences best linked topic\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    # Separates the sentences in the given para\n",
    "    passage = sent_tokenize(para)\n",
    "\n",
    "    # Loads the Bi-Encoder Model \n",
    "    bi_encoder = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "    bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "\n",
    "    # Loads the Cross-Encoder Reranker\n",
    "    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "    # embedding the paragraph and topic\n",
    "    corpus_embeddings = bi_encoder.encode(passage, convert_to_tensor=True, show_progress_bar=True)\n",
    "    question_embedding = bi_encoder.encode(topic, convert_to_tensor=True)\n",
    "    \n",
    "    # enables gpu if available\n",
    "    if torch.cuda.is_available():\n",
    "        question_embedding = question_embedding.cuda()\n",
    "\n",
    "    # Select 2 * k sentences from the para using the bi-encoder\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=k*2)\n",
    "    hits = hits[0]  \n",
    "\n",
    "    # Reranks the selected sentences and helps select k sentences\n",
    "    cross_inp = [[topic, passage[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "    results = []\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    for hit in hits[0:min(k,len(passage))]:\n",
    "        results.append(passage[hit['corpus_id']].lower())\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def find_keyphrase(para):\n",
    "\n",
    "    \"\"\"\n",
    "    Selects the best keyword from the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sent : string\n",
    "        Sentence\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    word : string\n",
    "        the best keyword from the text\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    # To avoid alpha and numeric values as the keywords\n",
    "    doc = nlp(para)\n",
    "    i = 0\n",
    "    while i < len(doc._.phrases):\n",
    "        word = doc._.phrases[0].text\n",
    "        word = re.sub(r'[^\\w\\s]', ' ', word)\n",
    "        temp = re.sub(' ', '', word)\n",
    "        if temp.isnumeric():\n",
    "            return word\n",
    "        elif temp.isalpha():\n",
    "            return word\n",
    "        i += 1\n",
    "    \n",
    "    return False\n",
    "\n",
    "def sent_ans_extractor(para, title , k=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes paragraph and it's topic as its input.\n",
    "    Extracts top 5 best sentences best linked to the topic.\n",
    "    Selects the best keyword from the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    para : string\n",
    "        Text for the pare\n",
    "    topic : string\n",
    "        Text for the topic\n",
    "    k : int\n",
    "        Number of sentences to be selected \n",
    "        (default value is 5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : list of strings\n",
    "        List of k sentences best linked topic\n",
    "          \n",
    "    \"\"\"\n",
    "    new_sents = []\n",
    "    words = []\n",
    "\n",
    "    sents = semanticsearch(para, title, k)\n",
    "    for i in sents:\n",
    "        word = find_keyphrase(i)\n",
    "        if word == False:\n",
    "            continue\n",
    "        new_sents.append(i)\n",
    "        words.append(word)\n",
    "    return new_sents, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"\"\"Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of machine learning and are at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.\n",
    "\n",
    "Artificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.\n",
    "\n",
    "Neural networks rely on training data to learn and improve their accuracy over time. However, once these learning algorithms are fine-tuned for accuracy, they are powerful tools in computer science and artificial intelligence, allowing us to classify and cluster data at a high velocity. Tasks in speech recognition or image recognition can take minutes versus hours when compared to the manual identification by human experts. One of the most well-known neural networks is Google’s search algorithm.\n",
    "\n",
    "Once an input layer is determined, weights are assigned. These weights help determine the importance of any given variable, with larger ones contributing more significantly to the output compared to other inputs. All inputs are then multiplied by their respective weights and then summed. Afterward, the output is passed through an activation function, which determines the output. If that output exceeds a given threshold, it “fires” (or activates) the node, passing data to the next layer in the network. This results in the output of one node becoming in the input of the next node. This process of passing data from one layer to the next layer defines this neural network as a feedforward network.\n",
    "\n",
    "If we use the activation function from the beginning of this section, we can determine that the output of this node would be 1, since 6 is greater than 0. In this instance, you would go surfing; but if we adjust the weights or the threshold, we can achieve different outcomes from the model. When we observe one decision, like in the above example, we can see how a neural network could make increasingly complex decisions depending on the output of previous decisions or layers.\n",
    "\n",
    "In the example above, we used perceptrons to illustrate some of the mathematics at play here, but neural networks leverage sigmoid neurons, which are distinguished by having values between 0 and 1. Since neural networks behave similarly to decision trees, cascading data from one node to another, having x values between 0 and 1 will reduce the impact of any given change of a single variable on the output of any given node, and subsequently, the output of the neural network.\n",
    "\n",
    "Neural networks can be classified into different types, which are used for different purposes. While this isn’t a comprehensive list of types, the below would be representative of the most common types of neural networks that you’ll come across for its common use cases:\n",
    "\n",
    "The perceptron is the oldest neural network, created by Frank Rosenblatt in 1958.\n",
    "\n",
    "Feedforward neural networks, or multi-layer perceptrons (MLPs), are what we’ve primarily been focusing on within this article. They are comprised of an input layer, a hidden layer or layers, and an output layer. While these neural networks are also commonly referred to as MLPs, it’s important to note that they are actually comprised of sigmoid neurons, not perceptrons, as most real-world problems are nonlinear. Data usually is fed into these models to train them, and they are the foundation for computer vision, natural language processing, and other neural networks.\n",
    "\n",
    "Convolutional neural networks (CNNs) are similar to feedforward networks, but they’re usually utilized for image recognition, pattern recognition, and/or computer vision. These networks harness principles from linear algebra, particularly matrix multiplication, to identify patterns within an image.\n",
    "\n",
    "Recurrent neural networks (RNNs) are identified by their feedback loops. These learning algorithms are primarily leveraged when using time-series data to make predictions about future outcomes, such as stock market predictions or sales forecasting.\n",
    "\n",
    "Deep Learning and neural networks tend to be used interchangeably in conversation, which can be confusing. As a result, it’s worth noting that the “deep” in deep learning is just referring to the depth of layers in a neural network. A neural network that consists of more than three layers—which would be inclusive of the inputs and the output—can be considered a deep learning algorithm. A neural network that only has two or three layers is just a basic neural network.\n",
    "\n",
    "\"\"\"\n",
    "title = 'what are neural networks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ans_extractor(para, title , k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WORKING VERSION ###\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import torch\n",
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "import pytextrank\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from QuestionGenerator import QuestionGenerator\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from sense2vec import Sense2Vec\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import wordnet as wn\n",
    "import random\n",
    "import pandas as pd \n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe(\"textrank\")\n",
    "nltk.download('punkt')\n",
    "\n",
    "def semanticsearch(para, topic, k=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes paragraph and it's topic as its input.\n",
    "    Extracts top 5 best sentences best linked to the topic.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    para : string\n",
    "        Text for the pare\n",
    "    topic : string\n",
    "        Text for the topic\n",
    "    k : int\n",
    "        Number of sentences to be selected \n",
    "        (default value is 5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : list of strings\n",
    "        List of k sentences best linked topic\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    # Separates the sentences in the given para\n",
    "    passage = sent_tokenize(para)\n",
    "\n",
    "    # Loads the Bi-Encoder Model \n",
    "    bi_encoder = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "    bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "\n",
    "    # Loads the Cross-Encoder Reranker\n",
    "    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "    # embedding the paragraph and topic\n",
    "    corpus_embeddings = bi_encoder.encode(passage, convert_to_tensor=True, show_progress_bar=True)\n",
    "    question_embedding = bi_encoder.encode(topic, convert_to_tensor=True)\n",
    "    \n",
    "    # enables gpu if available\n",
    "    if torch.cuda.is_available():\n",
    "        question_embedding = question_embedding.cuda()\n",
    "\n",
    "    # Select 2 * k sentences from the para using the bi-encoder\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=k*2)\n",
    "    hits = hits[0]  \n",
    "\n",
    "    # Reranks the selected sentences and helps select k sentences\n",
    "    cross_inp = [[topic, passage[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "    results = []\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    for hit in hits[0:min(k,len(passage))]:\n",
    "        results.append(passage[hit['corpus_id']].lower())\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def find_keyphrase(para):\n",
    "\n",
    "    \"\"\"\n",
    "    Selects the best keyword from the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sent : string\n",
    "        Sentence\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    word : string\n",
    "        the best keyword from the text\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    # To avoid alpha and numeric values as the keywords\n",
    "    doc = nlp(para)\n",
    "    i = 0\n",
    "    while i < len(doc._.phrases):\n",
    "        word = doc._.phrases[0].text\n",
    "        word = re.sub(r'[^\\w\\s]', ' ', word)\n",
    "        temp = re.sub(' ', '', word)\n",
    "        if temp.isnumeric():\n",
    "            return word\n",
    "        elif temp.isalpha():\n",
    "            return word\n",
    "        i += 1\n",
    "    \n",
    "    return False\n",
    "\n",
    "def sent_ans_extractor(para, topic , k=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes paragraph and it's topic as its input.\n",
    "    Extracts top 5 best sentences best linked to the topic.\n",
    "    Selects the best keyword from the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    para : string\n",
    "        Text for the pare\n",
    "    topic : string\n",
    "        Text for the topic\n",
    "    k : int\n",
    "        Number of sentences to be selected \n",
    "        (default value is 5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : list of strings\n",
    "        List of k sentences best linked topic\n",
    "          \n",
    "    \"\"\"\n",
    "    new_sents = []\n",
    "    words = []\n",
    "\n",
    "    sents = semanticsearch(para, topic, k)\n",
    "    for i in sents:\n",
    "        word = find_keyphrase(i)\n",
    "        if word == False:\n",
    "            continue\n",
    "        new_sents.append(i)\n",
    "        words.append(word)\n",
    "    return new_sents, words\n",
    "\n",
    "def question_generator(sentence, answer):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes the sentence and the answer as the input\n",
    "    to generate a question\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentece : string\n",
    "        Text for the sentence\n",
    "    answer : string\n",
    "        Text for the answer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    question : string\n",
    "        Text for the string\n",
    "          \n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    my_model_params = { \"MODEL_DIR\": \"./outputs/final/\", \n",
    "                    \"MAX_SOURCE_TEXT_LENGTH\": 75\n",
    "                    } \n",
    "\n",
    "    # encode text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(my_model_params[\"MODEL_DIR\"])\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': ['<answer>', '<context>']})\n",
    "\n",
    "    # using T5 with language model layer\n",
    "    model = T5ForConditionalGeneration.from_pretrained(my_model_params[\"MODEL_DIR\"])\n",
    "    model = model.to(device)  \n",
    "    \n",
    "    # prepare input\n",
    "    qg_input = f\"<answer> {answer} <context> {sentence}\"\n",
    "\n",
    "    # generate question\n",
    "    qg =  QuestionGenerator(model, tokenizer, device, max_input_length=75, max_output_length=25)\n",
    "    question = qg.generate(source_text=qg_input)\n",
    "    return question\n",
    "\n",
    "\n",
    "def find_related_word_online(word):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes word/phrase as an input and generates similar words/phrases \n",
    "    aka distractors using webscrapping from relatedwords.org website\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word : string\n",
    "        input words/phrases to generate distractors for\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    words : list of strings\n",
    "        List of distractors for the given input\n",
    "          \n",
    "    \"\"\"\n",
    "    r = requests.get(\"https://relatedwords.org/relatedto/\" + word)\n",
    "    soup = BeautifulSoup(r.content, 'html5lib') # If this line causes an error, run 'pip install html5lib' or install html5lib\n",
    "    sent = soup.prettify()[soup.prettify().find('\"terms\"'):]\n",
    "    words = []\n",
    "    count = 0\n",
    "    while count != 3:\n",
    "        ind1 = sent.find('\"word\":')+8\n",
    "        ind2 = sent[ind1:].find('\"')+ind1\n",
    "        words.append(sent[ind1:ind2])\n",
    "        sent = sent[ind2:]\n",
    "        count+=1\n",
    "    return words\n",
    "\n",
    "def sense2vec_get_words(word,s2v):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes word/phrase as an input and generates similar words/phrases \n",
    "    aka distractors using sense2vec\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word : string\n",
    "        input words/phrases to generate distractors for\n",
    "    s2v : Module instance from Sense2Vec class\n",
    "        Module instance from Sense2Vec class to generate distractor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distractors : list of strings\n",
    "        List of distractors for the given input\n",
    "          \n",
    "    \"\"\"\n",
    "    output = []\n",
    "    word = word.lower()\n",
    "    word = word.replace(\" \", \"_\")\n",
    "\n",
    "    sense = s2v.get_best_sense(word)\n",
    "\n",
    "    if sense == None:\n",
    "        return find_related_word_online(word)\n",
    "\n",
    "    most_similar = s2v.most_similar(sense, n=20)\n",
    " \n",
    "    for each_word in most_similar:\n",
    "        append_word = each_word[0].split(\"|\")[0].replace(\"_\", \" \").lower()\n",
    "        if append_word.lower() != word:\n",
    "            if sense.split(\"|\")[1] == each_word[0].split(\"|\")[1]:\n",
    "                output.append(append_word.title().lower())\n",
    "\n",
    "    out = list(OrderedDict.fromkeys(output))\n",
    "    return out\n",
    "\n",
    "def get_distractors(word):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes word/phrase as an input and generates similar words/phrases aka distractors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word : string\n",
    "        input words/phrases to generate distractors for\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distractors : list of strings\n",
    "        List of distractors for the given input\n",
    "          \n",
    "    \"\"\"\n",
    "    distractors = []\n",
    "    if word.isnumeric():\n",
    "        if len(word) == 4:\n",
    "            # if 4-digit number --> assume it's a year --> add/subtract random number btw 1-10\n",
    "            randomlist = random.sample(range(-10, 10), 3)\n",
    "            for num in randomlist:\n",
    "                distractors.append(str(int(word) + num))\n",
    "            return distractors\n",
    "        else:\n",
    "            # else if other number --> add/subtract random number --> don't change +ve or -ve\n",
    "            randomlist = random.sample(range(-1000, 1000), 3)\n",
    "            for num in randomlist:\n",
    "                distractors.append(str(int(word) + num))\n",
    "            return distractors\n",
    "\n",
    "    else:\n",
    "        word = word.lower()\n",
    "        s2v = Sense2Vec().from_disk('s2v_old')\n",
    "        distractors = sense2vec_get_words(word, s2v)\n",
    "        return distractors\n",
    "\n",
    "def distractor_generator(answer):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes word.phrase as an input and generates similar words/phrases aka distractors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    answer : string\n",
    "        input words/phrases to generate distractors for\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_distractors : list of strings\n",
    "        List of distractors for the given input\n",
    "          \n",
    "    \"\"\"\n",
    "    all_distractors = []\n",
    "    dis = {}\n",
    "    for word in answer.split(\" \"):\n",
    "        distractor = get_distractors(word)\n",
    "        dis[word] = distractor\n",
    "    \n",
    "    while len(all_distractors) < 3:\n",
    "        distr = \"\"\n",
    "        for word in dis:\n",
    "            rand_idx = int(random.random() * len(dis[word]))\n",
    "            distr += dis[word][rand_idx] + \" \"\n",
    "        if distr not in all_distractors:\n",
    "            all_distractors.append(distr[:-1])\n",
    "    return all_distractors\n",
    "\n",
    "def MCQ_generator(para, topic , k=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes paragraph and it's topic as its input.\n",
    "    Generates questions, correct and incorrect answers for MCQs\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    para : string\n",
    "        Text for the pare\n",
    "    topic : string\n",
    "        Text for the topic\n",
    "    k : int\n",
    "        Number of sentences to be selected \n",
    "        (default value is 5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : list of strings\n",
    "        List of k sentences best linked topic\n",
    "          \n",
    "    \"\"\"\n",
    "    sents, correct_ans = sent_ans_extractor(para, topic , k=5) # extracts the sentences and keywords\n",
    "    questions = []\n",
    "    all_ans = []\n",
    "    for i,j in zip(sents, correct_ans):\n",
    "        ques = question_generator(i, j) # generates the question\n",
    "        questions.append(ques)\n",
    "\n",
    "        temp = distractor_generator(j) # generates the distractors\n",
    "        temp.append(j)\n",
    "\n",
    "        random.shuffle(temp)\n",
    "        all_ans.append(temp)\n",
    "\n",
    "        # print('sentence :',i)\n",
    "        # print('question :',ques)\n",
    "        # print('all answer :',temp)\n",
    "        # print('correct answer:',j)\n",
    "        # print('\\n')\n",
    "\n",
    "    return questions, all_ans, correct_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mihir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import torch\n",
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "import pytextrank\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from QuestionGenerator import QuestionGenerator\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from sense2vec import Sense2Vec\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import wordnet as wn\n",
    "import random\n",
    "import pandas as pd \n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe(\"textrank\")\n",
    "nltk.download('punkt')\n",
    "\n",
    "def semanticsearch(para, topic, k=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes paragraph and it's topic as its input.\n",
    "    Extracts top 5 best sentences best linked to the topic.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    para : string\n",
    "        Text for the pare\n",
    "    topic : string\n",
    "        Text for the topic\n",
    "    k : int\n",
    "        Number of sentences to be selected \n",
    "        (default value is 5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : list of strings\n",
    "        List of k sentences best linked topic\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    # Separates the sentences in the given para\n",
    "    passage = sent_tokenize(para)\n",
    "\n",
    "    # Loads the Bi-Encoder Model \n",
    "    bi_encoder = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "    bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "\n",
    "    # Loads the Cross-Encoder Reranker\n",
    "    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "    # embedding the paragraph and topic\n",
    "    corpus_embeddings = bi_encoder.encode(passage, convert_to_tensor=True, show_progress_bar=True)\n",
    "    question_embedding = bi_encoder.encode(topic, convert_to_tensor=True)\n",
    "    \n",
    "    # enables gpu if available\n",
    "    if torch.cuda.is_available():\n",
    "        question_embedding = question_embedding.cuda()\n",
    "\n",
    "    # Select 2 * k sentences from the para using the bi-encoder\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=k*2)\n",
    "    hits = hits[0]  \n",
    "\n",
    "    # Reranks the selected sentences and helps select k sentences\n",
    "    cross_inp = [[topic, passage[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "    results = []\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    for hit in hits[0:min(k,len(passage))]:\n",
    "        results.append(passage[hit['corpus_id']].lower())\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def find_keyphrase(para):\n",
    "\n",
    "    \"\"\"\n",
    "    Selects the best keyword from the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sent : string\n",
    "        Sentence\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    word : string\n",
    "        the best keyword from the text\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    # To avoid alpha and numeric values as the keywords\n",
    "    doc = nlp(para)\n",
    "    i = 0\n",
    "    while i < len(doc._.phrases):\n",
    "        word = doc._.phrases[0].text\n",
    "        word = re.sub(r'[^\\w\\s]', ' ', word)\n",
    "        temp = re.sub(' ', '', word)\n",
    "        if temp.isnumeric():\n",
    "            return word\n",
    "        elif temp.isalpha():\n",
    "            return word\n",
    "        i += 1\n",
    "    \n",
    "    return False\n",
    "\n",
    "def sent_ans_extractor(para, topic , k=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes paragraph and it's topic as its input.\n",
    "    Extracts top 5 best sentences best linked to the topic.\n",
    "    Selects the best keyword from the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    para : string\n",
    "        Text for the pare\n",
    "    topic : string\n",
    "        Text for the topic\n",
    "    k : int\n",
    "        Number of sentences to be selected \n",
    "        (default value is 5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : list of strings\n",
    "        List of k sentences best linked topic\n",
    "          \n",
    "    \"\"\"\n",
    "    new_sents = []\n",
    "    words = []\n",
    "\n",
    "    sents = semanticsearch(para, topic, k)\n",
    "    for i in sents:\n",
    "        word = find_keyphrase(i)\n",
    "        if word == False:\n",
    "            continue\n",
    "        new_sents.append(i)\n",
    "        words.append(word)\n",
    "    return new_sents, words\n",
    "\n",
    "def question_generator(sentence, answer):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes the sentence and the answer as the input\n",
    "    to generate a question\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentece : string\n",
    "        Text for the sentence\n",
    "    answer : string\n",
    "        Text for the answer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    question : string\n",
    "        Text for the string\n",
    "          \n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    my_model_params = { \"MODEL_DIR\": \"./outputs/final/\", \n",
    "                    \"MAX_SOURCE_TEXT_LENGTH\": 75\n",
    "                    } \n",
    "\n",
    "    # encode text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(my_model_params[\"MODEL_DIR\"])\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': ['<answer>', '<context>']})\n",
    "\n",
    "    # using T5 with language model layer\n",
    "    model = T5ForConditionalGeneration.from_pretrained(my_model_params[\"MODEL_DIR\"])\n",
    "    model = model.to(device)  \n",
    "    \n",
    "    # prepare input\n",
    "    qg_input = f\"<answer> {answer} <context> {sentence}\"\n",
    "\n",
    "    # generate question\n",
    "    qg =  QuestionGenerator(model, tokenizer, device, max_input_length=75, max_output_length=25)\n",
    "    question = qg.generate(source_text=qg_input)\n",
    "    return question\n",
    "\n",
    "\n",
    "def find_related_word_online(word):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes word/phrase as an input and generates similar words/phrases \n",
    "    aka distractors using webscrapping from relatedwords.org website\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word : string\n",
    "        input words/phrases to generate distractors for\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    words : list of strings\n",
    "        List of distractors for the given input\n",
    "          \n",
    "    \"\"\"\n",
    "    r = requests.get(\"https://relatedwords.org/relatedto/\" + word)\n",
    "    soup = BeautifulSoup(r.content, 'html5lib') # If this line causes an error, run 'pip install html5lib' or install html5lib\n",
    "    sent = soup.prettify()[soup.prettify().find('\"terms\"'):]\n",
    "    words = []\n",
    "    count = 0\n",
    "    while count != 20:\n",
    "        ind1 = sent.find('\"word\":')+8\n",
    "        ind2 = sent[ind1:].find('\"')+ind1\n",
    "        words.append(sent[ind1:ind2])\n",
    "        sent = sent[ind2:]\n",
    "        count+=1\n",
    "    return words\n",
    "\n",
    "def sense2vec_get_words(word,s2v):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes word/phrase as an input and generates similar words/phrases \n",
    "    aka distractors using sense2vec\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word : string\n",
    "        input words/phrases to generate distractors for\n",
    "    s2v : Module instance from Sense2Vec class\n",
    "        Module instance from Sense2Vec class to generate distractor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distractors : list of strings\n",
    "        List of distractors for the given input\n",
    "          \n",
    "    \"\"\"\n",
    "    output = []\n",
    "    word = word.lower()\n",
    "    word = word.replace(\" \", \"_\")\n",
    "\n",
    "    sense = s2v.get_best_sense(word)\n",
    "\n",
    "    if sense == None:\n",
    "        return find_related_word_online(word)\n",
    "\n",
    "    most_similar = s2v.most_similar(sense, n=20)\n",
    " \n",
    "    for each_word in most_similar:\n",
    "        append_word = each_word[0].split(\"|\")[0].replace(\"_\", \" \").lower()\n",
    "        if append_word.lower() != word:\n",
    "            if sense.split(\"|\")[1] == each_word[0].split(\"|\")[1]:\n",
    "                output.append(append_word.title().lower())\n",
    "\n",
    "    out = list(OrderedDict.fromkeys(output))\n",
    "    return out\n",
    "\n",
    "def get_distractors(word):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes word/phrase as an input and generates similar words/phrases aka distractors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word : string\n",
    "        input words/phrases to generate distractors for\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distractors : list of strings\n",
    "        List of distractors for the given input\n",
    "          \n",
    "    \"\"\"\n",
    "    distractors = []\n",
    "    if word.isnumeric():\n",
    "        if len(word) == 4:\n",
    "            # if 4-digit number --> assume it's a year --> add/subtract random number btw 1-10\n",
    "            randomlist = random.sample(range(-10, 10), 20)\n",
    "            for num in randomlist:\n",
    "                distractors.append(str(int(word) + num))\n",
    "            return distractors\n",
    "        else:\n",
    "            # else if other number --> add/subtract random number --> don't change +ve or -ve\n",
    "            randomlist = random.sample(range(-1000, 1000), 20)\n",
    "            for num in randomlist:\n",
    "                distractors.append(str(int(word) + num))\n",
    "            return distractors\n",
    "\n",
    "    else:\n",
    "        word = word.lower()\n",
    "        s2v = Sense2Vec().from_disk('s2v_old')\n",
    "        distractors = sense2vec_get_words(word, s2v)\n",
    "        return distractors\n",
    "\n",
    "def distractor_generator(answer):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes word.phrase as an input and generates similar words/phrases aka distractors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    answer : string\n",
    "        input words/phrases to generate distractors for\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_distractors : list of strings\n",
    "        List of distractors for the given input\n",
    "          \n",
    "    \"\"\"\n",
    "    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "    all_distractors = []\n",
    "    dis = {}\n",
    "    for word in answer.split(\" \"):\n",
    "        distractor = get_distractors(word)\n",
    "        dis[word] = distractor\n",
    "    \n",
    "    while len(all_distractors) < 20:\n",
    "        distr = \"\"\n",
    "        for word in dis:\n",
    "            rand_idx = int(random.random() * len(dis[word]))\n",
    "            distr += dis[word][rand_idx] + \" \"\n",
    "        if not distr in all_distractors:\n",
    "            all_distractors.append(distr[:-1])\n",
    "\n",
    "    cross_inp = [[answer, all_distractors[i]] for i in range(len(all_distractors))]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    results = []\n",
    "    for i in sorted(range(len(cross_scores)), key=lambda i: cross_scores[i])[-3:]:\n",
    "        results.append(all_distractors[i])\n",
    "\n",
    "    return results\n",
    "\n",
    "def MCQ_generator(para, topic , k=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes paragraph and it's topic as its input.\n",
    "    Generates questions, correct and incorrect answers for MCQs\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    para : string\n",
    "        Text for the pare\n",
    "    topic : string\n",
    "        Text for the topic\n",
    "    k : int\n",
    "        Number of sentences to be selected \n",
    "        (default value is 5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : list of strings\n",
    "        List of k sentences best linked topic\n",
    "          \n",
    "    \"\"\"\n",
    "    sents, correct_ans = sent_ans_extractor(para, topic , k=5) # extracts the sentences and keywords\n",
    "    questions = []\n",
    "    all_ans = []\n",
    "    for i,j in zip(sents, correct_ans):\n",
    "        ques = question_generator(i, j) # generates the question\n",
    "        questions.append(ques)\n",
    "\n",
    "        temp = distractor_generator(j) # generates the distractors\n",
    "        temp.append(j)\n",
    "\n",
    "        random.shuffle(temp)\n",
    "        all_ans.append(temp)\n",
    "        \n",
    "        print('sentence :',i)\n",
    "        print('question :',ques)\n",
    "        print('all answer :',temp)\n",
    "        print('correct answer:',j)\n",
    "        print('\\n')\n",
    "\n",
    "    return questions, all_ans, correct_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "singing and dancing\n",
    "lead singer\n",
    "baby boy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['belting either danced', 'serenaded but dance', 'dancing either dances']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distractor_generator('singing and dancing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['leads lead guitarist', 'cause lead vocalist', 'results lead singer']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distractor_generator('lead singer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baby boy doll', 'baby boy other boy', 'newborn baby little boy']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distractor_generator('baby boy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"\"\"Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of machine learning and are at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.\n",
    "\n",
    "Artificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.\n",
    "\n",
    "Neural networks rely on training data to learn and improve their accuracy over time. However, once these learning algorithms are fine-tuned for accuracy, they are powerful tools in computer science and artificial intelligence, allowing us to classify and cluster data at a high velocity. Tasks in speech recognition or image recognition can take minutes versus hours when compared to the manual identification by human experts. One of the most well-known neural networks is Google’s search algorithm.\n",
    "\n",
    "Once an input layer is determined, weights are assigned. These weights help determine the importance of any given variable, with larger ones contributing more significantly to the output compared to other inputs. All inputs are then multiplied by their respective weights and then summed. Afterward, the output is passed through an activation function, which determines the output. If that output exceeds a given threshold, it “fires” (or activates) the node, passing data to the next layer in the network. This results in the output of one node becoming in the input of the next node. This process of passing data from one layer to the next layer defines this neural network as a feedforward network.\n",
    "\n",
    "If we use the activation function from the beginning of this section, we can determine that the output of this node would be 1, since 6 is greater than 0. In this instance, you would go surfing; but if we adjust the weights or the threshold, we can achieve different outcomes from the model. When we observe one decision, like in the above example, we can see how a neural network could make increasingly complex decisions depending on the output of previous decisions or layers.\n",
    "\n",
    "In the example above, we used perceptrons to illustrate some of the mathematics at play here, but neural networks leverage sigmoid neurons, which are distinguished by having values between 0 and 1. Since neural networks behave similarly to decision trees, cascading data from one node to another, having x values between 0 and 1 will reduce the impact of any given change of a single variable on the output of any given node, and subsequently, the output of the neural network.\n",
    "\n",
    "Neural networks can be classified into different types, which are used for different purposes. While this isn’t a comprehensive list of types, the below would be representative of the most common types of neural networks that you’ll come across for its common use cases:\n",
    "\n",
    "The perceptron is the oldest neural network, created by Frank Rosenblatt in 1958.\n",
    "\n",
    "Feedforward neural networks, or multi-layer perceptrons (MLPs), are what we’ve primarily been focusing on within this article. They are comprised of an input layer, a hidden layer or layers, and an output layer. While these neural networks are also commonly referred to as MLPs, it’s important to note that they are actually comprised of sigmoid neurons, not perceptrons, as most real-world problems are nonlinear. Data usually is fed into these models to train them, and they are the foundation for computer vision, natural language processing, and other neural networks.\n",
    "\n",
    "Convolutional neural networks (CNNs) are similar to feedforward networks, but they’re usually utilized for image recognition, pattern recognition, and/or computer vision. These networks harness principles from linear algebra, particularly matrix multiplication, to identify patterns within an image.\n",
    "\n",
    "Recurrent neural networks (RNNs) are identified by their feedback loops. These learning algorithms are primarily leveraged when using time-series data to make predictions about future outcomes, such as stock market predictions or sales forecasting.\n",
    "\n",
    "Deep Learning and neural networks tend to be used interchangeably in conversation, which can be confusing. As a result, it’s worth noting that the “deep” in deep learning is just referring to the depth of layers in a neural network. A neural network that consists of more than three layers—which would be inclusive of the inputs and the output—can be considered a deep learning algorithm. A neural network that only has two or three layers is just a basic neural network.\n",
    "\n",
    "\"\"\"\n",
    "title = 'what are neural networks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7128fde38879436c99413c955920315d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence : neural networks, also known as artificial neural networks (anns) or simulated neural networks (snns), are a subset of machine learning and are at the heart of deep learning algorithms.\n",
      "question : ['is a neural network at the heart of deep learning algorithms?']\n",
      "all answer : ['deeper programming machine learning', 'deepest learned machine learning', 'deepest learn computations', 'deep learning algorithms']\n",
      "correct answer: deep learning algorithms\n",
      "\n",
      "\n",
      "sentence : a neural network that only has two or three layers is just a basic neural network.\n",
      "question : ['a neural network that has two or three layers is just a basic neural network?']\n",
      "all answer : ['a neural network', 'that neuronal based network', 'an neuronal new network', 'that neuronal whole network']\n",
      "correct answer: a neural network\n",
      "\n",
      "\n",
      "sentence : artificial neural networks (anns) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer.\n",
      "question : ['is a layer with an input layer, hidden layers, and an output layer?']\n",
      "all answer : ['a output signal single layer', 'an output layer', 'that input/output solid layer', 'a input signal layers']\n",
      "correct answer: an output layer\n",
      "\n",
      "\n",
      "sentence : neural networks rely on training data to learn and improve their accuracy over time.\n",
      "question : ['do neural networks use training data to learn and improve their accuracy over time?']\n",
      "all answer : ['time', 'time-', 'time time', 'time-']\n",
      "correct answer: time\n",
      "\n",
      "\n",
      "sentence : convolutional neural networks (cnns) are similar to feedforward networks, but they’re usually utilized for image recognition, pattern recognition, and/or computer vision.\n",
      "question : ['are cnns similar to feedforward networks?']\n",
      "all answer : ['main computer clear vision', 'main computer clear vision', 'computer vision', 'windows computer direct vision']\n",
      "correct answer: computer vision\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([['is a neural network at the heart of deep learning algorithms?'],\n",
       "  ['a neural network that has two or three layers is just a basic neural network?'],\n",
       "  ['is a layer with an input layer, hidden layers, and an output layer?'],\n",
       "  ['do neural networks use training data to learn and improve their accuracy over time?'],\n",
       "  ['are cnns similar to feedforward networks?']],\n",
       " [['deeper programming machine learning',\n",
       "   'deepest learned machine learning',\n",
       "   'deepest learn computations',\n",
       "   'deep learning algorithms'],\n",
       "  ['a neural network',\n",
       "   'that neuronal based network',\n",
       "   'an neuronal new network',\n",
       "   'that neuronal whole network'],\n",
       "  ['a output signal single layer',\n",
       "   'an output layer',\n",
       "   'that input/output solid layer',\n",
       "   'a input signal layers'],\n",
       "  ['time', 'time-', 'time time', 'time-'],\n",
       "  ['main computer clear vision',\n",
       "   'main computer clear vision',\n",
       "   'computer vision',\n",
       "   'windows computer direct vision']],\n",
       " ['deep learning algorithms',\n",
       "  'a neural network',\n",
       "  'an output layer',\n",
       "  'time',\n",
       "  'computer vision'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCQ_generator(para, title, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "47c626452ef4ef3e74376d35c302fcf9bdc1b9327d6e04736eb914a557504e89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
