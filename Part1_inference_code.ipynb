{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mihir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import torch\n",
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "import pytextrank\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe(\"textrank\")\n",
    "nltk.download('punkt')\n",
    "\n",
    "def semanticsearch(para, topic, k=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes paragraph and it's topic as its input.\n",
    "    Extracts top 5 best sentences best linked to the topic.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    para : string\n",
    "        Text for the pare\n",
    "    topic : string\n",
    "        Text for the topic\n",
    "    k : int\n",
    "        Number of sentences to be selected \n",
    "        (default value is 5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : list of strings\n",
    "        List of k sentences best linked topic\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    # Separates the sentences in the given para\n",
    "    passage = sent_tokenize(para)\n",
    "\n",
    "    # Loads the Bi-Encoder Model \n",
    "    bi_encoder = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "    bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "\n",
    "    # Loads the Cross-Encoder Reranker\n",
    "    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "    # embedding the paragraph and topic\n",
    "    corpus_embeddings = bi_encoder.encode(passage, convert_to_tensor=True, show_progress_bar=True)\n",
    "    question_embedding = bi_encoder.encode(topic, convert_to_tensor=True)\n",
    "    \n",
    "    # enables gpu if available\n",
    "    if torch.cuda.is_available():\n",
    "        question_embedding = question_embedding.cuda()\n",
    "\n",
    "    # Select 2 * k sentences from the para using the bi-encoder\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=k*2)\n",
    "    hits = hits[0]  \n",
    "\n",
    "    # Reranks the selected sentences and helps select k sentences\n",
    "    cross_inp = [[topic, passage[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "    results = []\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    for hit in hits[0:min(k,len(passage))]:\n",
    "        results.append(passage[hit['corpus_id']].lower())\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def find_keyphrase(para):\n",
    "\n",
    "    \"\"\"\n",
    "    Selects the best keyword from the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sent : string\n",
    "        Sentence\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    word : string\n",
    "        the best keyword from the text\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    # To avoid alpha and numeric values as the keywords\n",
    "    doc = nlp(para)\n",
    "    i = 0\n",
    "    while i < len(doc._.phrases):\n",
    "        word = doc._.phrases[0].text\n",
    "        word = re.sub(r'[^\\w\\s]', ' ', word)\n",
    "        temp = re.sub(' ', '', word)\n",
    "        if temp.isnumeric():\n",
    "            return word\n",
    "        elif temp.isalpha():\n",
    "            return word\n",
    "        i += 1\n",
    "    \n",
    "    return False\n",
    "\n",
    "def sent_ans_extractor(para, title , k=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes paragraph and it's topic as its input.\n",
    "    Extracts top 5 best sentences best linked to the topic.\n",
    "    Selects the best keyword from the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    para : string\n",
    "        Text for the pare\n",
    "    topic : string\n",
    "        Text for the topic\n",
    "    k : int\n",
    "        Number of sentences to be selected \n",
    "        (default value is 5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : list of strings\n",
    "        List of k sentences best linked topic\n",
    "          \n",
    "    \"\"\"\n",
    "    new_sents = []\n",
    "    words = []\n",
    "\n",
    "    sents = semanticsearch(para, title, k)\n",
    "    for i in sents:\n",
    "        word = find_keyphrase(i)\n",
    "        if word == False:\n",
    "            continue\n",
    "        new_sents.append(i)\n",
    "        words.append(word)\n",
    "    return new_sents, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"\"\"Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of machine learning and are at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.\n",
    "\n",
    "Artificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.\n",
    "\n",
    "Neural networks rely on training data to learn and improve their accuracy over time. However, once these learning algorithms are fine-tuned for accuracy, they are powerful tools in computer science and artificial intelligence, allowing us to classify and cluster data at a high velocity. Tasks in speech recognition or image recognition can take minutes versus hours when compared to the manual identification by human experts. One of the most well-known neural networks is Google’s search algorithm.\n",
    "\n",
    "Once an input layer is determined, weights are assigned. These weights help determine the importance of any given variable, with larger ones contributing more significantly to the output compared to other inputs. All inputs are then multiplied by their respective weights and then summed. Afterward, the output is passed through an activation function, which determines the output. If that output exceeds a given threshold, it “fires” (or activates) the node, passing data to the next layer in the network. This results in the output of one node becoming in the input of the next node. This process of passing data from one layer to the next layer defines this neural network as a feedforward network.\n",
    "\n",
    "If we use the activation function from the beginning of this section, we can determine that the output of this node would be 1, since 6 is greater than 0. In this instance, you would go surfing; but if we adjust the weights or the threshold, we can achieve different outcomes from the model. When we observe one decision, like in the above example, we can see how a neural network could make increasingly complex decisions depending on the output of previous decisions or layers.\n",
    "\n",
    "In the example above, we used perceptrons to illustrate some of the mathematics at play here, but neural networks leverage sigmoid neurons, which are distinguished by having values between 0 and 1. Since neural networks behave similarly to decision trees, cascading data from one node to another, having x values between 0 and 1 will reduce the impact of any given change of a single variable on the output of any given node, and subsequently, the output of the neural network.\n",
    "\n",
    "Neural networks can be classified into different types, which are used for different purposes. While this isn’t a comprehensive list of types, the below would be representative of the most common types of neural networks that you’ll come across for its common use cases:\n",
    "\n",
    "The perceptron is the oldest neural network, created by Frank Rosenblatt in 1958.\n",
    "\n",
    "Feedforward neural networks, or multi-layer perceptrons (MLPs), are what we’ve primarily been focusing on within this article. They are comprised of an input layer, a hidden layer or layers, and an output layer. While these neural networks are also commonly referred to as MLPs, it’s important to note that they are actually comprised of sigmoid neurons, not perceptrons, as most real-world problems are nonlinear. Data usually is fed into these models to train them, and they are the foundation for computer vision, natural language processing, and other neural networks.\n",
    "\n",
    "Convolutional neural networks (CNNs) are similar to feedforward networks, but they’re usually utilized for image recognition, pattern recognition, and/or computer vision. These networks harness principles from linear algebra, particularly matrix multiplication, to identify patterns within an image.\n",
    "\n",
    "Recurrent neural networks (RNNs) are identified by their feedback loops. These learning algorithms are primarily leveraged when using time-series data to make predictions about future outcomes, such as stock market predictions or sales forecasting.\n",
    "\n",
    "Deep Learning and neural networks tend to be used interchangeably in conversation, which can be confusing. As a result, it’s worth noting that the “deep” in deep learning is just referring to the depth of layers in a neural network. A neural network that consists of more than three layers—which would be inclusive of the inputs and the output—can be considered a deep learning algorithm. A neural network that only has two or three layers is just a basic neural network.\n",
    "\n",
    "\"\"\"\n",
    "title = 'what are neural networks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1685576f9a5491abcf8108a7541d807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(['neural networks, also known as artificial neural networks (anns) or simulated neural networks (snns), are a subset of machine learning and are at the heart of deep learning algorithms.',\n",
       "  'a neural network that only has two or three layers is just a basic neural network.',\n",
       "  'artificial neural networks (anns) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer.',\n",
       "  'neural networks rely on training data to learn and improve their accuracy over time.',\n",
       "  'convolutional neural networks (cnns) are similar to feedforward networks, but they’re usually utilized for image recognition, pattern recognition, and/or computer vision.'],\n",
       " ['deep learning algorithms',\n",
       "  'a neural network',\n",
       "  'an output layer',\n",
       "  'time',\n",
       "  'computer vision'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_ans_extractor(para, title , k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mihir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import torch\n",
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "import pytextrank\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from QuestionGenerator import QuestionGenerator\n",
    "import random\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe(\"textrank\")\n",
    "nltk.download('punkt')\n",
    "\n",
    "def semanticsearch(para, topic, k=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes paragraph and it's topic as its input.\n",
    "    Extracts top 5 best sentences best linked to the topic.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    para : string\n",
    "        Text for the pare\n",
    "    topic : string\n",
    "        Text for the topic\n",
    "    k : int\n",
    "        Number of sentences to be selected \n",
    "        (default value is 5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : list of strings\n",
    "        List of k sentences best linked topic\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    # Separates the sentences in the given para\n",
    "    passage = sent_tokenize(para)\n",
    "\n",
    "    # Loads the Bi-Encoder Model \n",
    "    bi_encoder = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "    bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "\n",
    "    # Loads the Cross-Encoder Reranker\n",
    "    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "    # embedding the paragraph and topic\n",
    "    corpus_embeddings = bi_encoder.encode(passage, convert_to_tensor=True, show_progress_bar=True)\n",
    "    question_embedding = bi_encoder.encode(topic, convert_to_tensor=True)\n",
    "    \n",
    "    # enables gpu if available\n",
    "    if torch.cuda.is_available():\n",
    "        question_embedding = question_embedding.cuda()\n",
    "\n",
    "    # Select 2 * k sentences from the para using the bi-encoder\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=k*2)\n",
    "    hits = hits[0]  \n",
    "\n",
    "    # Reranks the selected sentences and helps select k sentences\n",
    "    cross_inp = [[topic, passage[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "    results = []\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    for hit in hits[0:min(k,len(passage))]:\n",
    "        results.append(passage[hit['corpus_id']].lower())\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def find_keyphrase(para):\n",
    "\n",
    "    \"\"\"\n",
    "    Selects the best keyword from the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sent : string\n",
    "        Sentence\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    word : string\n",
    "        the best keyword from the text\n",
    "          \n",
    "    \"\"\"\n",
    "\n",
    "    # To avoid alpha and numeric values as the keywords\n",
    "    doc = nlp(para)\n",
    "    i = 0\n",
    "    while i < len(doc._.phrases):\n",
    "        word = doc._.phrases[0].text\n",
    "        word = re.sub(r'[^\\w\\s]', ' ', word)\n",
    "        temp = re.sub(' ', '', word)\n",
    "        if temp.isnumeric():\n",
    "            return word\n",
    "        elif temp.isalpha():\n",
    "            return word\n",
    "        i += 1\n",
    "    \n",
    "    return False\n",
    "\n",
    "def sent_ans_extractor(para, topic , k=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes paragraph and it's topic as its input.\n",
    "    Extracts top 5 best sentences best linked to the topic.\n",
    "    Selects the best keyword from the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    para : string\n",
    "        Text for the pare\n",
    "    topic : string\n",
    "        Text for the topic\n",
    "    k : int\n",
    "        Number of sentences to be selected \n",
    "        (default value is 5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : list of strings\n",
    "        List of k sentences best linked topic\n",
    "          \n",
    "    \"\"\"\n",
    "    new_sents = []\n",
    "    words = []\n",
    "\n",
    "    sents = semanticsearch(para, topic, k)\n",
    "    for i in sents:\n",
    "        word = find_keyphrase(i)\n",
    "        if word == False:\n",
    "            continue\n",
    "        new_sents.append(i)\n",
    "        words.append(word)\n",
    "    return new_sents, words\n",
    "\n",
    "def question_generator(sentence, answer):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes the sentence and the answer as the input\n",
    "    to generate a question\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentece : string\n",
    "        Text for the sentence\n",
    "    answer : string\n",
    "        Text for the answer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    question : string\n",
    "        Text for the string\n",
    "          \n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    my_model_params = { \"MODEL_DIR\": \"./outputs/final/\", \n",
    "                    \"MAX_SOURCE_TEXT_LENGTH\": 75\n",
    "                    } \n",
    "\n",
    "    # encode text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(my_model_params[\"MODEL_DIR\"])\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': ['<answer>', '<context>']})\n",
    "\n",
    "    # using T5 with language model layer\n",
    "    model = T5ForConditionalGeneration.from_pretrained(my_model_params[\"MODEL_DIR\"])\n",
    "    model = model.to(device)  \n",
    "    \n",
    "    # prepare input\n",
    "    qg_input = f\"<answer> {answer} <context> {sentence}\"\n",
    "\n",
    "    # generate question\n",
    "    qg =  QuestionGenerator(model, tokenizer, device, max_input_length=75, max_output_length=25)\n",
    "    question = qg.generate(source_text=qg_input)\n",
    "    return question\n",
    "\n",
    "def MCQ_generator(para, topic , k=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes paragraph and it's topic as its input.\n",
    "    Generates questions, correct and incorrect answers for MCQs\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    para : string\n",
    "        Text for the pare\n",
    "    topic : string\n",
    "        Text for the topic\n",
    "    k : int\n",
    "        Number of sentences to be selected \n",
    "        (default value is 5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : list of strings\n",
    "        List of k sentences best linked topic\n",
    "          \n",
    "    \"\"\"\n",
    "    sents, correct_ans = sent_ans_extractor(para, topic , k=5)\n",
    "    questions = []\n",
    "    all_ans = []\n",
    "    for i,j in zip(sents, correct_ans):\n",
    "        ques = question_generator(i, j)\n",
    "        print('sentence :',i)\n",
    "        print('question :',ques)\n",
    "        print('answer :',j)\n",
    "        questions.append(ques)\n",
    "        temp = [j]\n",
    "\n",
    "        # add distractor her\n",
    "\n",
    "        random.shuffle(temp)\n",
    "        all_ans.append(temp)\n",
    "\n",
    "    return questions, all_ans, correct_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"\"\"Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of machine learning and are at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.\n",
    "\n",
    "Artificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.\n",
    "\n",
    "Neural networks rely on training data to learn and improve their accuracy over time. However, once these learning algorithms are fine-tuned for accuracy, they are powerful tools in computer science and artificial intelligence, allowing us to classify and cluster data at a high velocity. Tasks in speech recognition or image recognition can take minutes versus hours when compared to the manual identification by human experts. One of the most well-known neural networks is Google’s search algorithm.\n",
    "\n",
    "Once an input layer is determined, weights are assigned. These weights help determine the importance of any given variable, with larger ones contributing more significantly to the output compared to other inputs. All inputs are then multiplied by their respective weights and then summed. Afterward, the output is passed through an activation function, which determines the output. If that output exceeds a given threshold, it “fires” (or activates) the node, passing data to the next layer in the network. This results in the output of one node becoming in the input of the next node. This process of passing data from one layer to the next layer defines this neural network as a feedforward network.\n",
    "\n",
    "If we use the activation function from the beginning of this section, we can determine that the output of this node would be 1, since 6 is greater than 0. In this instance, you would go surfing; but if we adjust the weights or the threshold, we can achieve different outcomes from the model. When we observe one decision, like in the above example, we can see how a neural network could make increasingly complex decisions depending on the output of previous decisions or layers.\n",
    "\n",
    "In the example above, we used perceptrons to illustrate some of the mathematics at play here, but neural networks leverage sigmoid neurons, which are distinguished by having values between 0 and 1. Since neural networks behave similarly to decision trees, cascading data from one node to another, having x values between 0 and 1 will reduce the impact of any given change of a single variable on the output of any given node, and subsequently, the output of the neural network.\n",
    "\n",
    "Neural networks can be classified into different types, which are used for different purposes. While this isn’t a comprehensive list of types, the below would be representative of the most common types of neural networks that you’ll come across for its common use cases:\n",
    "\n",
    "The perceptron is the oldest neural network, created by Frank Rosenblatt in 1958.\n",
    "\n",
    "Feedforward neural networks, or multi-layer perceptrons (MLPs), are what we’ve primarily been focusing on within this article. They are comprised of an input layer, a hidden layer or layers, and an output layer. While these neural networks are also commonly referred to as MLPs, it’s important to note that they are actually comprised of sigmoid neurons, not perceptrons, as most real-world problems are nonlinear. Data usually is fed into these models to train them, and they are the foundation for computer vision, natural language processing, and other neural networks.\n",
    "\n",
    "Convolutional neural networks (CNNs) are similar to feedforward networks, but they’re usually utilized for image recognition, pattern recognition, and/or computer vision. These networks harness principles from linear algebra, particularly matrix multiplication, to identify patterns within an image.\n",
    "\n",
    "Recurrent neural networks (RNNs) are identified by their feedback loops. These learning algorithms are primarily leveraged when using time-series data to make predictions about future outcomes, such as stock market predictions or sales forecasting.\n",
    "\n",
    "Deep Learning and neural networks tend to be used interchangeably in conversation, which can be confusing. As a result, it’s worth noting that the “deep” in deep learning is just referring to the depth of layers in a neural network. A neural network that consists of more than three layers—which would be inclusive of the inputs and the output—can be considered a deep learning algorithm. A neural network that only has two or three layers is just a basic neural network.\n",
    "\n",
    "\"\"\"\n",
    "title = 'what are neural networks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d170873baffb4af8aff36b0830755bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence : neural networks, also known as artificial neural networks (anns) or simulated neural networks (snns), are a subset of machine learning and are at the heart of deep learning algorithms.\n",
      "question : ['is a neural network at the heart of deep learning algorithms?']\n",
      "answer : deep learning algorithms\n",
      "sentence : a neural network that only has two or three layers is just a basic neural network.\n",
      "question : ['a neural network that has two or three layers is just a basic neural network?']\n",
      "answer : a neural network\n",
      "sentence : artificial neural networks (anns) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer.\n",
      "question : ['is a layer with an input layer, hidden layers, and an output layer?']\n",
      "answer : an output layer\n",
      "sentence : neural networks rely on training data to learn and improve their accuracy over time.\n",
      "question : ['do neural networks use training data to learn and improve their accuracy over time?']\n",
      "answer : time\n",
      "sentence : convolutional neural networks (cnns) are similar to feedforward networks, but they’re usually utilized for image recognition, pattern recognition, and/or computer vision.\n",
      "question : ['are cnns similar to feedforward networks?']\n",
      "answer : computer vision\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([['is a neural network at the heart of deep learning algorithms?'],\n",
       "  ['a neural network that has two or three layers is just a basic neural network?'],\n",
       "  ['is a layer with an input layer, hidden layers, and an output layer?'],\n",
       "  ['do neural networks use training data to learn and improve their accuracy over time?'],\n",
       "  ['are cnns similar to feedforward networks?']],\n",
       " [['deep learning algorithms'],\n",
       "  ['a neural network'],\n",
       "  ['an output layer'],\n",
       "  ['time'],\n",
       "  ['computer vision']],\n",
       " ['deep learning algorithms',\n",
       "  'a neural network',\n",
       "  'an output layer',\n",
       "  'time',\n",
       "  'computer vision'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCQ_generator(para, title, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "47c626452ef4ef3e74376d35c302fcf9bdc1b9327d6e04736eb914a557504e89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
