{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import tokenize\n",
    "df = pd.read_csv('sample_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_text = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    ls_text.append(tokenize.sent_tokenize(df['context'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Transformers method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "This type of symbiosis is relatively uncommon in rudimentary reference texts, but is omnipresent in the natural world.\n",
      "An example of mutual symbiosis is the relationship between the ocellaris clownfish that dwell among the tentacles of Ritteri sea anemones.\n",
      "Another non-obligate symbiosis is known from encrusting bryozoans and hermit crabs that live in a close relationship.\n",
      "Symbiotic relationships include those associations in which one organism lives on another (ectosymbiosis, such as mistletoe), or where one partner lives inside the other (endosymbiosis, such as lactobacilli and other bacteria in humans or Symbiodinium in corals).\n",
      "While historically, symbiosis has received less attention than other interactions such as predation or competition, it is increasingly recognized as an important selective force behind evolution, with many species having a long history of interdependent co-evolution.\n",
      "Many biologists restrict the definition of symbiosis to close mutualist relationships.\n",
      "An example of a biotrophic relationship would be a tick feeding on the blood of its host.\n",
      "Some believe symbiosis should only refer to persistent mutualisms, while others believe it should apply to any type of persistent biological interaction (in other words mutualistic, commensalistic, or parasitic).\n",
      "During mutualistic symbioses, the host cell lacks some of the nutrients, which are provided by the endosymbiont.\n",
      "Symbiosis (from Greek σύν \"together\" and βίωσις \"living\") is close and often long-term interaction between two different biological species.\n",
      "\n",
      "\n",
      "\n",
      "The origins of the Samoans are closely studied in modern research about Polynesia in various scientific disciplines such as genetics, linguistics and anthropology.\n",
      "Rugby union is the national sport in Samoa and the national team, nicknamed the Manu Samoa, is consistently competitive against teams from vastly more populous nations.\n",
      "When Christianity was introduced in Samoa, most Samoan people converted.\n",
      "The Samoan culture is centred around the principle of vāfealoa'i, the relationships between people.\n",
      "Samoa also played in the Pacific Nations Cup and the Pacific Tri-Nations The sport is governed by the Samoa Rugby Football Union, who are members of the Pacific Islands Rugby Alliance, and thus, also contribute to the international Pacific Islanders rugby union team.\n",
      "Samoa also signed a friendship treaty with New Zealand.\n",
      "While all of the islands have volcanic origins, only Savai'i, the western most island in Samoa, is volcanically active with the most recent eruptions in Mt Matavanu (1905–1911), Mata o le Afi (1902) and Mauga Afi (1725).\n",
      "Some Samoans are spiritual and religious, and have subtly adapted the dominant religion of Christianity to 'fit in' with fa'a Samoa and vice versa.\n",
      "Other noteworthy players from NZ and Australia have represented the Samoan National team.\n",
      "The Independent State of Samoa ( Samoan: Malo Sa 'oloto Tuto 'atasi o Sāmoa, IPA: [ˌsaːˈmoa]), commonly known as Samoa (Samoan: Sāmoa) and formerly known as Western Samoa, is a Unitary Parliamentary Republic with eleven administrative divisions.\n",
      "\n",
      "\n",
      "\n",
      "[citation needed] With house music already massive on the '80s dance-scene it was only a matter of time before it would penetrate the UK pop charts.\n",
      "Fingers\", claims that the term \"house\" became popular due to many of the early DJs creating music in their own homes using synthesizers and drum machines such as the Roland TR-808, TR-909, and the TB 303.\n",
      "Key labels in the rise of house music in the UK included: The tour in March 1987[citation needed] of Knuckles, Jefferson, Fingers Inc. (Heard) and Adonis as the DJ International Tour boosted house in the UK.\n",
      "House music proved to be a commercially successful genre and a more mainstream pop-based variation grew increasingly popular.\n",
      "In an effort to maintain such exclusives, the DJs were inspired to create their own \"house\" records.\n",
      "The single is credited as helping to bring house music to the US mainstream.\n",
      "Since the early to mid-1990s, house music has been infused in mainstream pop and dance music worldwide.\n",
      "International record label, doesn't mention Importes Etc., Frankie Knuckles, or the Warehouse by name, but agrees that \"house\" was a regional catch-all term for dance music, and that it was once synonymous with older disco music.\n",
      "2010s saw multiple new sounds in house music developed by numerous DJs.\n",
      "House music is a genre of electronic dance music that originated in Chicago in the early 1980s.\n",
      "\n",
      "\n",
      "\n",
      "Although most Czech vocabulary is based on shared roots with Slavic, Romance, and Germanic languages, many loanwords (most associated with high culture) have been adopted in recent years.\n",
      "By then the language had developed a literary tradition, and since then it has changed little; journals from that period have no substantial differences from modern standard Czech, and contemporary Czechs can understand them with little difficulty.\n",
      "The Czechs' language separated from other Slavic tongues into what would later be called Old Czech by the thirteenth century, a classification extending through the sixteenth century.\n",
      "Czech typographical features not associated with phonetics generally resemble those of most Latin European languages, including English.\n",
      "Czech continued to evolve and gain in regional importance for hundreds of years, and has been a literary language in the Slovak lands since the early fifteenth century.\n",
      "Slovak has slightly more borrowed words than Czech.\n",
      "The consensus among linguists is that modern, standard Czech originated during the eighteenth century.\n",
      "These changes differentiated Czech from Slovak.\n",
      "Since then, \"Czechoslovak\" refers to improvised pidgins of the languages which have arisen from the decrease in mutual intelligibility.\n",
      "Czech has one of the most phonemic orthographies of all European languages.\n",
      "\n",
      "\n",
      "\n",
      "Italians in different regions today may also speak regional varieties of standard Italian, or regional Italian dialects, which, unlike the majority of languages of Italy, are actually dialects of standard Italian rather than separate languages.\n",
      "These regional languages are often referred to colloquially or in non-linguistic circles as Italian \"dialects,\" or dialetti (standard Italian for \"dialects\").\n",
      "In addition to having evolved, for the most part, separately from one another and with distinct individual histories, the Latin-based regional Romance languages of Italy are also better classified as separate languages rather than true \"dialects\" due to the often high degree in which they lack mutual intelligibility.\n",
      "Meanwhile, the \"dialects\" subordinate to the standard language are generally not variations on the standard language but rather separate (but often related) languages in and of themselves.\n",
      "For example, most of the various regional Romance languages of Italy, often colloquially referred to as Italian \"dialects,\" are, in fact, not actually derived from modern standard Italian, but rather evolved from Vulgar Latin separately and individually from one another and independently of standard Italian, long prior to the diffusion of a national standardized language throughout what is now Italy.\n",
      "These various Latin-derived regional languages are therefore, in a linguistic sense, not truly \"dialects\" of the standard Italian language, but are instead better defined as their own separate languages.\n",
      "There may be multiple standard dialects associated with a single language.\n",
      "In this secondary sense of \"dialect\", language varieties are often called dialects rather than languages: The status of \"language\" is not solely determined by linguistic criteria, but it is also the result of a historical and political development.\n",
      "They are therefore better classified as individual languages rather than \"dialects.\"\n",
      "By the definition most commonly used by linguists, any linguistic variety can be considered a \"dialect\" of some language—\"everybody speaks a dialect\".\n",
      "\n",
      "\n",
      "\n",
      "Hanover is of national importance because of its universities and medical school, its international airport and its large zoo.\n",
      "Around 40 theatres are located in Hanover.\n",
      "The Theater für Niedersachsen is another big theatre in Hanover, which also has an own Musical-Company.\n",
      "\"Hanover\" is the traditional English spelling.\n",
      "A large Jewish population then existed in Hanover.\n",
      "Every year Hanover hosts the Schützenfest Hannover, the world's largest marksmen's festival, and the Oktoberfest Hannover, the second largest Oktoberfest in the world (beside Oktoberfest of Blumenau).\n",
      "Hanover was thus a gateway to the Rhine, Ruhr and Saar river valleys, their industrial areas which grew up to the southwest and the plains regions to the east and north, for overland traffic skirting the Harz between the Low Countries and Saxony or Thuringia.\n",
      "But Hanover is not only one of the most important Exhibition Cities in the world, it is also one of the German capitals for marksmen.\n",
      "[citation needed] Some other popular sights are the Waterloo Column, the Laves House, the Wangenheim Palace, the Lower Saxony State Archives, the Hanover Playhouse, the Kröpcke Clock, the Anzeiger Tower Block, the Administration Building of the NORD/LB, the Cupola Hall of the Congress Centre, the Lower Saxony Stock, the Ministry of Finance, the Garten Church, the Luther Church, the Gehry Tower (designed by the American architect Frank O. Gehry), the specially designed Bus Stops, the Opera House, the Central Station, the Maschsee lake and the city forest Eilenriede, which is one of the largest of its kind in Europe.\n",
      "With a population of 518,000, Hanover is a major centre of Northern Germany and the country's thirteenth largest city.\n",
      "\n",
      "\n",
      "\n",
      "Partially digested food fills the duodenum.\n",
      "This allows the mass of food to further mix with the digestive enzymes.\n",
      "In the small intestine, the larger part of digestion takes place and this is helped by the secretions of bile, pancreatic juice and intestinal juice.\n",
      "In chemical digestion, enzymes break down food into the small molecules the body can use.\n",
      "The stomach continues to break food down mechanically and chemically through churning and mixing with both acids and enzymes.\n",
      "It is stimulated by distension of the stomach, presence of food in stomach and decrease in pH.\n",
      "Food is formed into a bolus by the mechanical mastication and swallowed into the esophagus from where it enters the stomach through the action of peristalsis.\n",
      "Digesta is finally moved into the small intestine, where the digestion and absorption of nutrients occurs.\n",
      "In the stomach further release of enzymes break down the food further and this is combined with the churning action of the stomach.\n",
      "Digestion begins in the mouth with the secretion of saliva and its digestive enzymes.\n",
      "\n",
      "\n",
      "\n",
      "The following year, IBM hosted its first Invention Award Dinner honoring 34 outstanding IBM inventors; and in 1963, the company named the first eight IBM Fellows in a new Fellowship Program that recognizes senior IBM scientists, engineers and other professionals for outstanding technical achievements.\n",
      "On September 21, 1953, Thomas Watson, Jr., the company's president at the time, sent out a controversial letter to all IBM employees stating that IBM needed to hire the best people, regardless of their race, ethnic origin, or gender.\n",
      "The company stated that this would give IBM a competitive advantage because IBM would then be able to hire talented people its competitors would turn down.\n",
      "IBM has been a leading proponent of the Open Source Initiative, and began supporting Linux in 1998.\n",
      "IBM's employee management practices can be traced back to its roots.\n",
      "As of 2012[update], IBM had been the top annual recipient of U.S. patents for 20 consecutive years.\n",
      "The initialism IBM followed.\n",
      "IBM acquired Kenexa (2012) and SPSS (2009) and PwC's consulting business (2002), spinning off companies like printer manufacturer Lexmark (1991), and selling off product lines like its personal computer and x86 server businesses to Lenovo (2005, 2014).\n",
      "IBM has constantly evolved since its inception.\n",
      "The program searches for fresh start-up companies that IBM can partner with to solve world problems.\n",
      "\n",
      "\n",
      "\n",
      "The start of Neolithic 1 overlaps the Tahunian and Heavy Neolithic periods to some degree.\n",
      "However, excavations in Central Europe have revealed that early Neolithic Linear Ceramic cultures (\"Linearbandkeramik\") were building large arrangements of circular ditches between 4800 BC and 4600 BC.\n",
      "Not all of these cultural elements characteristic of the Neolithic appeared everywhere in the same order: the earliest farming societies in the Near East did not use pottery.\n",
      "In southeast Europe agrarian societies first appeared in the 7th millennium BC, attested by one of the earliest farming sites of Europe, discovered in Vashtëmi, southeastern Albania and dating back to 6,500 BC.\n",
      "Around 10,200 BC the first fully developed Neolithic cultures belonging to the phase Pre-Pottery Neolithic A (PPNA) appeared in the fertile crescent.\n",
      "The beginning of the Neolithic culture is considered to be in the Levant (Jericho, modern-day West Bank) about 10,200 – 8,800 BC.\n",
      "The shelter of the early people changed dramatically from the paleolithic to the neolithic era.\n",
      "Neolithic people were skilled farmers, manufacturing a range of tools necessary for the tending, harvesting and processing of crops (such as sickle blades and grinding stones) and food production (e.g.\n",
      "The Neolithic is a progression of behavioral and cultural characteristics and changes, including the use of wild and domestic crops and of domesticated animals.\n",
      "Neolithic peoples in the Levant, Anatolia, Syria, northern Mesopotamia and Central Asia were also accomplished builders, utilizing mud-brick to construct houses and villages.\n",
      "\n",
      "\n",
      "\n",
      "According to psychologists, sexual orientation also refers to a person’s choice of sexual partners, who may be homosexual, heterosexual, or bisexual.\n",
      "Some researchers who study sexual orientation argue that the concept may not apply similarly to men and women.\n",
      "Sexual orientation is argued as a concept that evolved in the industrialized West, and there is a controversy as to the universality of its application in other societies or cultures.\n",
      "Research suggests that sexual orientation is independent of cultural and other social influences, but that open identification of one's sexual orientation may be hindered by homophobic/hetereosexist settings.\n",
      "Influences of culture may complicate the process of measuring sexual orientation.\n",
      "Furthermore, there are more than two dimensions of sexuality to be considered.\n",
      "They additionally state that sexual orientation \"is distinct from other components of sex and gender, including biological sex (the anatomical, physiological, and genetic characteristics associated with being male or female), gender identity (the psychological sense of being male or female), and social gender role (the cultural norms that define feminine and masculine behavior)\".\n",
      "Individuals may or may not consider their sexual orientation to define their sexual identity, as they may experience various degrees of fluidity of sexuality, or may simply identify more strongly with another aspect of their identity such as family role.\n",
      "Research over several decades has demonstrated that sexual orientation ranges along a continuum, from exclusive attraction to the opposite sex to exclusive attraction to the same sex.\n",
      "Individuals may or may not express their sexual orientation in their behaviors.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "ans_1 = []\n",
    "scores = []\n",
    "for k in range(len(df)):\n",
    "    print('\\n\\n')\n",
    "    #Compute embedding for both lists\n",
    "    embeddings1 = model.encode(ls_text[k], convert_to_tensor=True)\n",
    "\n",
    "    a = np.matmul(embeddings1,embeddings1.T)\n",
    "    a = np.abs(a).numpy()\n",
    "    a = np.sum(a, axis = 1)\n",
    "    temp = []\n",
    "    temp2 = []\n",
    "    for i in np.argpartition(a, -10)[-10:]:\n",
    "        # print(ls_text[k][i])\n",
    "        temp.append(ls_text[k][i])\n",
    "        print(ls_text[k][i])\n",
    "        temp2.append(a[i])\n",
    "\n",
    "        \n",
    "    # for i in np.argpartition(a, -10)[-10:]:\n",
    "    #     # print(ls_text[k][i])\n",
    "    #     temp.append(ls_text[k][i])\n",
    "    #     # print(ls_text[k][i])\n",
    "    #     print(a[i])\n",
    "\n",
    "    ans_1.append(temp)\n",
    "    scores.append(temp2)\n",
    "\n",
    "\n",
    "    #Compute cosine-similarities\n",
    "    # cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "    # #Output the pairs with their score\n",
    "    # for i in range(len(sentences1)):\n",
    "    #     print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(ans_1).T\n",
    "df1.to_csv('sent_trans_1.csv')\n",
    "df1 = pd.DataFrame(scores).T\n",
    "df1.to_csv('sent_trans_1_scores.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Transformers method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Symbiosis\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Samoa\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: House_music\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Czech_language\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Dialect\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Hanover\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Digestion\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: IBM\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Neolithic\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Sexual_orientation\n",
      "\n",
      "Top 10 most similar sentences in corpus:\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "ans_2 = []\n",
    "scores = []\n",
    "for i in range(len(df)):\n",
    "    sents = ls_text[i]\n",
    "    \n",
    "    # Query sentences:\n",
    "    queries = [df['title'][i]]\n",
    "    corpus_embeddings = embedder.encode(sents, convert_to_tensor=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Find the closest 10 sentences of the corpus for each query sentence based on cosine similarity\n",
    "\n",
    "    top_k = min(10, len(sents))\n",
    "    for query in queries:\n",
    "        query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "        # We use cosine-similarity and torch.topk to find the highest 1- scores\n",
    "        cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "        top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "        print(\"\\n\\n======================\\n\\n\")\n",
    "        print(\"Query:\", query)\n",
    "        print(\"\\nTop 10 most similar sentences in corpus:\")\n",
    "        \n",
    "        temp = []\n",
    "        temp2 = []\n",
    "        for score, idx in zip(top_results[0], top_results[1]):\n",
    "            # print(sents[idx], \"(Score: {:.4f})\".format(score))\n",
    "            temp.append(sents[idx])\n",
    "            temp2.append(float(\"{:.4f}\".format(score)))\n",
    "        ans_2.append(temp)\n",
    "        scores.append(temp2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7749, 0.7628, 0.7476, 0.6967, 0.6674, 0.6501, 0.646, 0.6346, 0.6177, 0.6036]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(ans_2).T\n",
    "# df2.to_csv('sent_trans_2.csv')\n",
    "df2 = pd.DataFrame(scores).T\n",
    "df2.to_csv('sent_trans_2_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_frequency_matrix(sentences):\n",
    "    frequency_matrix = {}\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    lemma = WordNetLemmatizer()\n",
    "\n",
    "    for sent in sentences:\n",
    "        freq_table = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = lemma.lemmatize(word)\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        frequency_matrix[sent[:15]] = freq_table\n",
    "\n",
    "    return frequency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_tf_matrix(freq_matrix):\n",
    "    tf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        tf_table = {}\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, count in f_table.items():\n",
    "            tf_table[word] = count / count_words_in_sentence\n",
    "\n",
    "        tf_matrix[sent] = tf_table\n",
    "\n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_documents_per_words(freq_matrix):\n",
    "    word_per_doc_table = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        for word, count in f_table.items():\n",
    "            if word in word_per_doc_table:\n",
    "                word_per_doc_table[word] += 1\n",
    "            else:\n",
    "                word_per_doc_table[word] = 1\n",
    "\n",
    "    return word_per_doc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
    "    idf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        idf_table = {}\n",
    "\n",
    "        for word in f_table.keys():\n",
    "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
    "\n",
    "        idf_matrix[sent] = idf_table\n",
    "\n",
    "    return idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "\n",
    "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "\n",
    "        tf_idf_table = {}\n",
    "\n",
    "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
    "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "\n",
    "        tf_idf_matrix[sent1] = tf_idf_table\n",
    "\n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _score_sentences(tf_idf_matrix) -> dict:\n",
    "    \"\"\"\n",
    "    score a sentence by its word's TF\n",
    "    Basic algorithm: adding the TF frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    sentenceValue = {}\n",
    "\n",
    "    for sent, f_table in tf_idf_matrix.items():\n",
    "        total_score_per_sentence = 0\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, score in f_table.items():\n",
    "            total_score_per_sentence += score\n",
    "\n",
    "        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n",
    "\n",
    "    return sentenceValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_average_score(sentenceValue) -> int:\n",
    "    \"\"\"\n",
    "    Find the average score from the sentence value dictionary\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    sumValues = 0\n",
    "    for entry in sentenceValue:\n",
    "        sumValues += sentenceValue[entry]\n",
    "\n",
    "    # Average value of a sentence from original summary_text\n",
    "    average = (sumValues / len(sentenceValue))\n",
    "\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_summary(sentences, sentenceValue, threshold):\n",
    "    sentence_count = 0\n",
    "    summary = []\n",
    "    sco = []\n",
    "\n",
    "    ls = np.argpartition(list(sentenceValue.values()), -10)[-10:]\n",
    "    threshold = list(sentenceValue.values())[ls[0]]\n",
    "    # ans = []\n",
    "    # for i in ls:\n",
    "    #     ans.append(sentences)\n",
    "    for sentence in sentences:\n",
    "        if sentence[:15] in sentenceValue and sentenceValue[sentence[:15]] >= (threshold):\n",
    "            summary.append(sentence)\n",
    "            sentence_count += 1\n",
    "            sco.append(sentenceValue[sentence[:15]])\n",
    "\n",
    "    return summary,sco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mihir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords    \n",
    "    \n",
    "'''\n",
    "We already have a sentence tokenizer, so we just need \n",
    "to run the sent_tokenize() method to create the array of sentences.\n",
    "'''\n",
    "\n",
    "ans_3 = []\n",
    "scores = []\n",
    "\n",
    "for i in range(len(ls_text)):\n",
    "\n",
    "    sentences = ls_text[i]\n",
    "    # 1 Sentence Tokenize\n",
    "    # sentences = sent_tokenize(text)\n",
    "    total_documents = len(sentences)\n",
    "    #print(sentences)\n",
    "\n",
    "    # 2 Create the Frequency matrix of the words in each sentence.\n",
    "    freq_matrix = _create_frequency_matrix(sentences)\n",
    "    #print(freq_matrix)\n",
    "\n",
    "    '''\n",
    "    Term frequency (TF) is how often a word appears in a document, divided by how many words are there in a document.\n",
    "    '''\n",
    "    # 3 Calculate TermFrequency and generate a matrix\n",
    "    tf_matrix = _create_tf_matrix(freq_matrix)\n",
    "    #print(tf_matrix)\n",
    "\n",
    "    # 4 creating table for documents per words\n",
    "    count_doc_per_words = _create_documents_per_words(freq_matrix)\n",
    "    #print(count_doc_per_words)\n",
    "\n",
    "    '''\n",
    "    Inverse document frequency (IDF) is how unique or rare a word is.\n",
    "    '''\n",
    "    # 5 Calculate IDF and generate a matrix\n",
    "    idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
    "    #print(idf_matrix)\n",
    "\n",
    "    # 6 Calculate TF-IDF and generate a matrix\n",
    "    tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
    "    #print(tf_idf_matrix)\n",
    "\n",
    "    # 7 Important Algorithm: score the sentences\n",
    "    sentence_scores = _score_sentences(tf_idf_matrix)\n",
    "    #print(sentence_scores)\n",
    "\n",
    "    # 8 Find the threshold\n",
    "    threshold = _find_average_score(sentence_scores)\n",
    "    #print(threshold)\n",
    "\n",
    "    # 9 Important Algorithm: Generate the summary\n",
    "    summary,sco = _generate_summary(sentences, sentence_scores, threshold)\n",
    "    # print(summary)\n",
    "    ans_3.append(summary)\n",
    "    scores.append(sco)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(ans_3).T\n",
    "df2.to_csv('tf-idf.csv')\n",
    "df2 = pd.DataFrame(scores).T\n",
    "df2.to_csv('tf-idf_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence_transformer_method1'] = ans_1\n",
    "df['sentence_transformer_method2'] = ans_2\n",
    "df['tf_idf'] = ans_3\n",
    "df.to_csv('sample_text_results_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\"Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of machine learning and are at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.\n",
    "\n",
    "Artificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.\n",
    "\n",
    "Neural networks rely on training data to learn and improve their accuracy over time. However, once these learning algorithms are fine-tuned for accuracy, they are powerful tools in computer science and artificial intelligence, allowing us to classify and cluster data at a high velocity. Tasks in speech recognition or image recognition can take minutes versus hours when compared to the manual identification by human experts. One of the most well-known neural networks is Google’s search algorithm.\n",
    "\n",
    "Once an input layer is determined, weights are assigned. These weights help determine the importance of any given variable, with larger ones contributing more significantly to the output compared to other inputs. All inputs are then multiplied by their respective weights and then summed. Afterward, the output is passed through an activation function, which determines the output. If that output exceeds a given threshold, it “fires” (or activates) the node, passing data to the next layer in the network. This results in the output of one node becoming in the input of the next node. This process of passing data from one layer to the next layer defines this neural network as a feedforward network.\n",
    "\n",
    "If we use the activation function from the beginning of this section, we can determine that the output of this node would be 1, since 6 is greater than 0. In this instance, you would go surfing; but if we adjust the weights or the threshold, we can achieve different outcomes from the model. When we observe one decision, like in the above example, we can see how a neural network could make increasingly complex decisions depending on the output of previous decisions or layers.\n",
    "\n",
    "In the example above, we used perceptrons to illustrate some of the mathematics at play here, but neural networks leverage sigmoid neurons, which are distinguished by having values between 0 and 1. Since neural networks behave similarly to decision trees, cascading data from one node to another, having x values between 0 and 1 will reduce the impact of any given change of a single variable on the output of any given node, and subsequently, the output of the neural network.\n",
    "\n",
    "Neural networks can be classified into different types, which are used for different purposes. While this isn’t a comprehensive list of types, the below would be representative of the most common types of neural networks that you’ll come across for its common use cases:\n",
    "\n",
    "The perceptron is the oldest neural network, created by Frank Rosenblatt in 1958.\n",
    "\n",
    "Feedforward neural networks, or multi-layer perceptrons (MLPs), are what we’ve primarily been focusing on within this article. They are comprised of an input layer, a hidden layer or layers, and an output layer. While these neural networks are also commonly referred to as MLPs, it’s important to note that they are actually comprised of sigmoid neurons, not perceptrons, as most real-world problems are nonlinear. Data usually is fed into these models to train them, and they are the foundation for computer vision, natural language processing, and other neural networks.\n",
    "\n",
    "Convolutional neural networks (CNNs) are similar to feedforward networks, but they’re usually utilized for image recognition, pattern recognition, and/or computer vision. These networks harness principles from linear algebra, particularly matrix multiplication, to identify patterns within an image.\n",
    "\n",
    "Recurrent neural networks (RNNs) are identified by their feedback loops. These learning algorithms are primarily leveraged when using time-series data to make predictions about future outcomes, such as stock market predictions or sales forecasting.\n",
    "\n",
    "Deep Learning and neural networks tend to be used interchangeably in conversation, which can be confusing. As a result, it’s worth noting that the “deep” in deep learning is just referring to the depth of layers in a neural network. A neural network that consists of more than three layers—which would be inclusive of the inputs and the output—can be considered a deep learning algorithm. A neural network that only has two or three layers is just a basic neural network.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('networks', 0.37653801069261195), ('network', 0.37653801069261195), ('layers', 0.2578096552459131), ('layer', 0.2578096552459131), ('neural', 0.25425390792625246), ('data', 0.247348041933658), ('decision', 0.16509210413479947), ('decisions', 0.16509210413479947), ('neurons', 0.15753549557120922), ('neuron', 0.15753549557120922)]\n"
     ]
    }
   ],
   "source": [
    "TR_keywords = keywords.keywords(summary, scores=True)\n",
    "print(TR_keywords[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keybert\n",
      "  Downloading keybert-0.5.1.tar.gz (19 kB)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from keybert) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from keybert) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from keybert) (1.21.5)\n",
      "Collecting rich>=10.4.0\n",
      "  Downloading rich-12.5.1-py3-none-any.whl (235 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (2.11.2)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.8.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (3.7)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.64.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (1.12.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.1.96)\n",
      "Requirement already satisfied: torchvision in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.13.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (4.1.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.0.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.3.15)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.12.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers>=0.3.8->keybert) (0.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2021.10.8)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (9.0.1)\n",
      "Building wheels for collected packages: keybert\n",
      "  Building wheel for keybert (setup.py): started\n",
      "  Building wheel for keybert (setup.py): finished with status 'done'\n",
      "  Created wheel for keybert: filename=keybert-0.5.1-py3-none-any.whl size=21332 sha256=7f4da0a0827a80cbf122ed41c197f6d28d1deb5a869f7f4cff100b95bf72ddd3\n",
      "  Stored in directory: c:\\users\\mihir\\appdata\\local\\pip\\cache\\wheels\\94\\18\\2a\\f26bbcd25924aab452bb4bcc2345a55c07160823d196a264c7\n",
      "Successfully built keybert\n",
      "Installing collected packages: commonmark, rich, keybert\n",
      "Successfully installed commonmark-0.9.1 keybert-0.5.1 rich-12.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model = KeyBERT(model='all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neural networks google', 'neural network', 'processing neural networks', 'neural networks snns', 'neural networks anns', 'layer network neural', 'network neural network', 'network neural', 'algorithm neural', 'neural network layers']\n"
     ]
    }
   ],
   "source": [
    "keywords = kw_model.extract_keywords(summary, \n",
    "\n",
    "                                     keyphrase_ngram_range=(1, 3), \n",
    "\n",
    "                                     stop_words='english', \n",
    "\n",
    "                                     highlight=False,\n",
    "\n",
    "                                     top_n=10)\n",
    "\n",
    "keywords_list= list(dict(keywords).keys())\n",
    "\n",
    "print(keywords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cdj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks\n",
      "0.12890204814099 3\n",
      "[Neural networks, Neural networks, Neural networks]\n",
      "neural networks\n",
      "0.12890204814099 4\n",
      "[neural networks, neural networks, neural networks, neural networks]\n",
      "other neural networks\n",
      "0.12553041726409025 1\n",
      "[other neural networks]\n",
      "Artificial neural networks\n",
      "0.12399304051033513 1\n",
      "[Artificial neural networks]\n",
      "artificial neural networks\n",
      "0.12399304051033513 1\n",
      "[artificial neural networks]\n",
      "simulated neural networks\n",
      "0.1204046985684943 1\n",
      "[simulated neural networks]\n",
      "Convolutional neural networks\n",
      "0.11919226027186872 1\n",
      "[Convolutional neural networks]\n",
      "Feedforward neural networks\n",
      "0.11919226027186872 1\n",
      "[Feedforward neural networks]\n",
      "Recurrent neural networks\n",
      "0.11919226027186872 1\n",
      "[Recurrent neural networks]\n",
      "feedforward networks\n",
      "0.10376433508421891 1\n",
      "[feedforward networks]\n",
      "layers\n",
      "0.09455237544804192 3\n",
      "[layers, layers, layers]\n",
      "deep learning algorithms\n",
      "0.07252282768046249 1\n",
      "[deep learning algorithms]\n",
      "the oldest neural network\n",
      "0.0714232241661447 1\n",
      "[the oldest neural network]\n",
      "other inputs\n",
      "0.06976785887225666 1\n",
      "[other inputs]\n",
      "cluster data\n",
      "0.06914004889119205 1\n",
      "[cluster data]\n",
      "A neural network\n",
      "0.06836063426199662 2\n",
      "[A neural network, A neural network]\n",
      "a neural network\n",
      "0.06836063426199662 2\n",
      "[a neural network, a neural network]\n",
      "the neural network\n",
      "0.06836063426199662 1\n",
      "[the neural network]\n",
      "these neural networks\n",
      "0.06836063426199662 1\n",
      "[these neural networks]\n",
      "this neural network\n",
      "0.06836063426199662 1\n",
      "[this neural network]\n",
      "computer vision\n",
      "0.06622562190546394 2\n",
      "[computer vision, computer vision]\n",
      "Data\n",
      "0.06590218379319385 1\n",
      "[Data]\n",
      "data\n",
      "0.06590218379319385 4\n",
      "[data, data, data, data]\n",
      "image recognition\n",
      "0.06452147303272297 2\n",
      "[image recognition, image recognition]\n",
      "artificial neuron\n",
      "0.061780315984398745 1\n",
      "[artificial neuron]\n",
      "pattern recognition\n",
      "0.061622084779876116 1\n",
      "[pattern recognition]\n",
      "deep learning\n",
      "0.061482377155051594 1\n",
      "[deep learning]\n",
      "computer science\n",
      "0.061368113310988986 1\n",
      "[computer science]\n",
      "an output layer\n",
      "0.06046706017968348 2\n",
      "[an output layer, an output layer]\n",
      "stock market predictions\n",
      "0.05992312165532928 1\n",
      "[stock market predictions]\n",
      "natural language processing\n",
      "0.05931597307642598 1\n",
      "[natural language processing]\n",
      "a node layers\n",
      "0.05851083461901183 1\n",
      "[a node layers]\n",
      "different outcomes\n",
      "0.05822768065379225 1\n",
      "[different outcomes]\n",
      "sigmoid neurons\n",
      "0.056839646534558405 2\n",
      "[sigmoid neurons, sigmoid neurons]\n",
      "biological neurons\n",
      "0.056117967373619765 1\n",
      "[biological neurons]\n",
      "sales forecasting\n",
      "0.05600669637107989 1\n",
      "[sales forecasting]\n",
      "different types\n",
      "0.05525164950930065 1\n",
      "[different types]\n",
      "a feedforward network\n",
      "0.05502934873752329 1\n",
      "[a feedforward network]\n",
      "an input layer\n",
      "0.05499622977173083 3\n",
      "[an input layer, an input layer, an input layer]\n",
      "the next layer\n",
      "0.05327229697231126 4\n",
      "[the next layer, the next layer, the next layer, the next layer]\n",
      "just a basic neural network\n",
      "0.05276956533829753 1\n",
      "[just a basic neural network]\n",
      "human experts\n",
      "0.05204605943246213 1\n",
      "[human experts]\n",
      "multi-layer perceptrons\n",
      "0.051055238760987054 1\n",
      "[multi-layer perceptrons]\n",
      "artificial intelligence\n",
      "0.050833395418729374 1\n",
      "[artificial intelligence]\n",
      "previous decisions\n",
      "0.0507195856323264 1\n",
      "[previous decisions]\n",
      "future outcomes\n",
      "0.05069437209084172 1\n",
      "[future outcomes]\n",
      "speech recognition\n",
      "0.05050627183985015 1\n",
      "[speech recognition]\n",
      "a hidden layer\n",
      "0.05050412752686391 1\n",
      "[a hidden layer]\n",
      "threshold\n",
      "0.0497152886324815 1\n",
      "[threshold]\n",
      "machine learning\n",
      "0.04958679669542311 1\n",
      "[machine learning]\n",
      "different purposes\n",
      "0.049302503644659296 1\n",
      "[different purposes]\n",
      "weights\n",
      "0.049192333368885795 1\n",
      "[weights]\n",
      "These networks\n",
      "0.04817755964687074 1\n",
      "[These networks]\n",
      "the network\n",
      "0.04817755964687074 3\n",
      "[the network, the network, the network]\n",
      "powerful tools\n",
      "0.04631216998784985 1\n",
      "[powerful tools]\n",
      "values\n",
      "0.046026980080428645 1\n",
      "[values]\n",
      "perceptrons\n",
      "0.04595371495796072 2\n",
      "[perceptrons, perceptrons]\n",
      "predictions\n",
      "0.04584483336282883 1\n",
      "[predictions]\n",
      "Frank Rosenblatt\n",
      "0.04418597408411322 2\n",
      "[Frank Rosenblatt, Frank Rosenblatt]\n",
      "time-series data\n",
      "0.0438464883436562 1\n",
      "[time-series data]\n",
      "one layer\n",
      "0.043671871533956456 1\n",
      "[one layer]\n",
      "time\n",
      "0.04325889305988464 1\n",
      "[time]\n",
      "any given node\n",
      "0.042754971319384376 1\n",
      "[any given node]\n",
      "accuracy\n",
      "0.04250189252330018 1\n",
      "[accuracy]\n",
      "the next node\n",
      "0.042324691860493646 1\n",
      "[the next node]\n",
      "patterns\n",
      "0.04150466805750804 1\n",
      "[patterns]\n",
      "a deep learning algorithm\n",
      "0.04069457062044282 1\n",
      "[a deep learning algorithm]\n",
      "one or more hidden layers\n",
      "0.040379344895781016 1\n",
      "[one or more hidden layers]\n",
      "larger ones\n",
      "0.03969560841254051 1\n",
      "[larger ones]\n",
      "types\n",
      "0.03877658780392279 1\n",
      "[types]\n",
      "the specified threshold value\n",
      "0.0371344049685171 1\n",
      "[the specified threshold value]\n",
      "more than three layers\n",
      "0.03656155452000443 1\n",
      "[more than three layers]\n",
      "the most well-known neural networks\n",
      "0.0365452840774448 1\n",
      "[the most well-known neural networks]\n",
      "any individual node\n",
      "0.03593281381456073 1\n",
      "[any individual node]\n",
      "a given threshold\n",
      "0.03545998041987098 1\n",
      "[a given threshold]\n",
      "that output\n",
      "0.03482500179040378 1\n",
      "[that output]\n",
      "the output\n",
      "0.03482500179040378 10\n",
      "[the output, the output, the output, the output, the output, the output, the output, the output, the output, the output]\n",
      "these learning algorithms\n",
      "0.03445287187746785 1\n",
      "[these learning algorithms]\n",
      "linear algebra\n",
      "0.03440678350979695 1\n",
      "[linear algebra]\n",
      "Google’s search algorithm\n",
      "0.032575931099935264 1\n",
      "[Google’s search algorithm]\n",
      "any given variable\n",
      "0.03208245671400878 1\n",
      "[any given variable]\n",
      "These learning algorithms\n",
      "0.031917131729108636 1\n",
      "[These learning algorithms]\n",
      "minutes\n",
      "0.031889803026131244 1\n",
      "[minutes]\n",
      "Each node\n",
      "0.03184602746427984 1\n",
      "[Each node]\n",
      "one node\n",
      "0.03184602746427984 2\n",
      "[one node, one node]\n",
      "that node\n",
      "0.03184602746427984 1\n",
      "[that node]\n",
      "the node\n",
      "0.03184602746427984 1\n",
      "[the node]\n",
      "this node\n",
      "0.03184602746427984 1\n",
      "[this node]\n",
      "hours\n",
      "0.03166561031737754 1\n",
      "[hours]\n",
      "ANNs\n",
      "0.03158362883862567 2\n",
      "[ANNs, ANNs]\n",
      "most real-world problems\n",
      "0.030297019420454447 1\n",
      "[most real-world problems]\n",
      "any given change\n",
      "0.02970994315734814 1\n",
      "[any given change]\n",
      "an associated weight\n",
      "0.028681938265862086 1\n",
      "[an associated weight]\n",
      "MLPs\n",
      "0.02818736365267501 2\n",
      "[MLPs, MLPs]\n",
      "play\n",
      "0.02793088823445404 1\n",
      "[play]\n",
      "their respective weights\n",
      "0.027124545848445997 1\n",
      "[their respective weights]\n",
      "trees\n",
      "0.026600092744780174 1\n",
      "[trees]\n",
      "its common use cases\n",
      "0.026537850091410176 1\n",
      "[its common use cases]\n",
      "increasingly complex decisions\n",
      "0.0264416691223613 1\n",
      "[increasingly complex decisions]\n",
      "conversation\n",
      "0.026396203342373185 1\n",
      "[conversation]\n",
      "SNNs\n",
      "0.026004197425956332 1\n",
      "[SNNs]\n",
      "two or three layers\n",
      "0.02599086455126528 1\n",
      "[two or three layers]\n",
      "All inputs\n",
      "0.025956638523209216 1\n",
      "[All inputs]\n",
      "the input\n",
      "0.025956638523209216 1\n",
      "[the input]\n",
      "the inputs\n",
      "0.025956638523209216 1\n",
      "[the inputs]\n",
      "a single variable\n",
      "0.025248961219678512 1\n",
      "[a single variable]\n",
      "the human brain\n",
      "0.024725364797833313 1\n",
      "[the human brain]\n",
      "a high velocity\n",
      "0.02412256316085894 1\n",
      "[a high velocity]\n",
      "the manual identification\n",
      "0.02387252612963144 1\n",
      "[the manual identification]\n",
      "CNNs\n",
      "0.023800604889540215 1\n",
      "[CNNs]\n",
      "RNNs\n",
      "0.023800604889540215 1\n",
      "[RNNs]\n",
      "Google\n",
      "0.023789682718487044 1\n",
      "[Google]\n",
      "their feedback loops\n",
      "0.023583730555710187 1\n",
      "[their feedback loops]\n",
      "the threshold\n",
      "0.022962508219842512 1\n",
      "[the threshold]\n",
      "These weights\n",
      "0.02272096553007362 1\n",
      "[These weights]\n",
      "the weights\n",
      "0.02272096553007362 1\n",
      "[the weights]\n",
      "Deep Learning\n",
      "0.02251178157196829 2\n",
      "[Deep Learning, Deep Learning]\n",
      "minutes versus hours\n",
      "0.022470371680378676 1\n",
      "[minutes versus hours]\n",
      "principles\n",
      "0.022428281627288063 1\n",
      "[principles]\n",
      "an image\n",
      "0.02133713639753242 1\n",
      "[an image]\n",
      "x values\n",
      "0.021258951471536818 1\n",
      "[x values]\n",
      "The perceptron\n",
      "0.021225111760993565 1\n",
      "[The perceptron]\n",
      "one decision\n",
      "0.020966847641886485 1\n",
      "[one decision]\n",
      "particularly matrix multiplication\n",
      "0.020092460621139892 1\n",
      "[particularly matrix multiplication]\n",
      "their accuracy\n",
      "0.019630783271516725 1\n",
      "[their accuracy]\n",
      "an activation function\n",
      "0.018985524964015593 1\n",
      "[an activation function]\n",
      "the activation function\n",
      "0.018985524964015593 1\n",
      "[the activation function]\n",
      "the most common types\n",
      "0.01871619453161301 1\n",
      "[the most common types]\n",
      "this article\n",
      "0.018134943931364347 1\n",
      "[this article]\n",
      "structure\n",
      "0.017900835271694823 1\n",
      "[structure]\n",
      "the model\n",
      "0.017552400559874306 1\n",
      "[the model]\n",
      "these models\n",
      "0.017552400559874306 1\n",
      "[these models]\n",
      "the above example\n",
      "0.016819448978292198 1\n",
      "[the above example]\n",
      "Tasks\n",
      "0.015801517168065198 1\n",
      "[Tasks]\n",
      "the way\n",
      "0.015274866018142068 1\n",
      "[the way]\n",
      "the impact\n",
      "0.01463423597313193 1\n",
      "[the impact]\n",
      "the foundation\n",
      "0.014328632159533318 1\n",
      "[the foundation]\n",
      "the heart\n",
      "0.013338815381547263 1\n",
      "[the heart]\n",
      "the importance\n",
      "0.012922127970796268 1\n",
      "[the importance]\n",
      "the depth\n",
      "0.012732006960511506 1\n",
      "[the depth]\n",
      "the mathematics\n",
      "0.01235804906029754 1\n",
      "[the mathematics]\n",
      "a subset\n",
      "0.01210918459148582 1\n",
      "[a subset]\n",
      "a comprehensive list\n",
      "0.011938675054596852 1\n",
      "[a comprehensive list]\n",
      "this section\n",
      "0.011787430049680263 1\n",
      "[this section]\n",
      "more than three\n",
      "0.011670987444510172 1\n",
      "[more than three]\n",
      "the example\n",
      "0.011648141190167095 1\n",
      "[the example]\n",
      "the beginning\n",
      "0.011458957701991873 1\n",
      "[the beginning]\n",
      "the below\n",
      "0.010864240007812491 1\n",
      "[the below]\n",
      "Their name\n",
      "0.007298408152469549 1\n",
      "[Their name]\n",
      "This process\n",
      "0.007298408152469549 1\n",
      "[This process]\n",
      "a result\n",
      "0.007298408152469549 1\n",
      "[a result]\n",
      "no data\n",
      "0.007298408152469549 1\n",
      "[no data]\n",
      "this instance\n",
      "0.007298408152469549 1\n",
      "[this instance]\n",
      "1\n",
      "0.0 2\n",
      "[1, 1]\n",
      "1958\n",
      "0.0 1\n",
      "[1958]\n",
      "6\n",
      "0.0 1\n",
      "[6]\n",
      "One\n",
      "0.0 1\n",
      "[One]\n",
      "They\n",
      "0.0 1\n",
      "[They]\n",
      "another\n",
      "0.0 2\n",
      "[another, another]\n",
      "it\n",
      "0.0 3\n",
      "[it, it, it]\n",
      "one\n",
      "0.0 4\n",
      "[one, one, one, one]\n",
      "some\n",
      "0.0 1\n",
      "[some]\n",
      "that\n",
      "0.0 4\n",
      "[that, that, that, that]\n",
      "them\n",
      "0.0 1\n",
      "[them]\n",
      "they\n",
      "0.0 4\n",
      "[they, they, they, they]\n",
      "this\n",
      "0.0 1\n",
      "[this]\n",
      "three\n",
      "0.0 1\n",
      "[three]\n",
      "two\n",
      "0.0 1\n",
      "[two]\n",
      "us\n",
      "0.0 1\n",
      "[us]\n",
      "we\n",
      "0.0 8\n",
      "[we, we, we, we, we, we, we, we]\n",
      "what\n",
      "0.0 1\n",
      "[what]\n",
      "which\n",
      "0.0 5\n",
      "[which, which, which, which, which]\n",
      "you\n",
      "0.0 2\n",
      "[you, you]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### Pytextrank to find important phrases\n",
    "\n",
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "# example text\n",
    "# text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\"\n",
    "text = summary\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# examine the top-ranked phrases in the document\n",
    "for phrase in doc._.phrases:\n",
    "    print(phrase.text)\n",
    "    print(phrase.rank, phrase.count)\n",
    "    print(phrase.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "47c626452ef4ef3e74376d35c302fcf9bdc1b9327d6e04736eb914a557504e89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
