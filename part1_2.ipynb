{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning is a hot topic these days.\n",
      "Neural networks are multi-layer networks of neurons (the blue and magenta nodes in the chart below) that we use to classify things, make predictions, etc.\n",
      "Below is the diagram of a simple neural network with five inputs, 5 outputs, and two hidden layers of neurons.\n",
      "And by the end, hopefully you (and I) will have gained a deeper and more intuitive understanding of how neural networks do what they do.\n",
      "In this post, we will explore the ins and outs of a simple neural network.\n",
      "Our second hidden layer of neurons in magenta.\n",
      "Neural networks are the workhorses of deep learning.\n",
      "The arrows that connect the dots shows how all the neurons are interconnected and how data travels from the input layer all the way through to the output layer.\n",
      "To even begin to answer it, we will need to learn the basics of neural networks.\n",
      "We will also watch how the neural network learns from its mistake using a process known as backpropagation.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = ['The cat sits outside',\n",
    "             'A man is playing guitar',\n",
    "             'The new movie is awesome']\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sents, convert_to_tensor=True)\n",
    "\n",
    "a = np.matmul(embeddings1,embeddings1.T)\n",
    "a = np.abs(a).numpy()\n",
    "a = np.sum(a, axis = 1)\n",
    "for i in np.argpartition(a, -10)[-10:]:\n",
    "    print(sents[i])\n",
    "#Compute cosine-similarities\n",
    "# cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "# #Output the pairs with their score\n",
    "# for i in range(len(sentences1)):\n",
    "#     print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(embeddings1[0], embeddings1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data.df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good morning Dr. Adams.', 'The patient is waiting for you in room number 3.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "p = \"Good morning Dr. Adams. The patient is waiting for you in room number 3.\"\n",
    "\n",
    "tokenize.sent_tokenize(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "understanding neural networks\n"
     ]
    }
   ],
   "source": [
    "# i = \n",
    "para = \"\"\"Deep learning is a hot topic these days. But what is it that makes it special and sets it apart from other aspects of machine learning? That is a deep question (pardon the pun). To even begin to answer it, we will need to learn the basics of neural networks.\n",
    "\n",
    "Neural networks are the workhorses of deep learning. And while they may look like black boxes, deep down (sorry, I will stop the terrible puns) they are trying to accomplish the same thing as any other model — to make good predictions.\n",
    "\n",
    "In this post, we will explore the ins and outs of a simple neural network. And by the end, hopefully you (and I) will have gained a deeper and more intuitive understanding of how neural networks do what they do.\n",
    "Let’s start with a really high level overview so we know what we are working with. Neural networks are multi-layer networks of neurons (the blue and magenta nodes in the chart below) that we use to classify things, make predictions, etc. Below is the diagram of a simple neural network with five inputs, 5 outputs, and two hidden layers of neurons.\n",
    "Starting from the left, we have:\n",
    "\n",
    "The input layer of our model in orange.\n",
    "Our first hidden layer of neurons in blue.\n",
    "Our second hidden layer of neurons in magenta.\n",
    "The output layer (aka the prediction) of our model in green.\n",
    "The arrows that connect the dots shows how all the neurons are interconnected and how data travels from the input layer all the way through to the output layer.\n",
    "\n",
    "Later we will calculate step by step each output value. We will also watch how the neural network learns from its mistake using a process known as backpropagation.\n",
    "\"\"\"\n",
    "title = 'understanding neural networks'\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deep learning is a hot topic these days.',\n",
       " 'But what is it that makes it special and sets it apart from other aspects of machine learning?',\n",
       " 'That is a deep question (pardon the pun).',\n",
       " 'To even begin to answer it, we will need to learn the basics of neural networks.',\n",
       " 'Neural networks are the workhorses of deep learning.',\n",
       " 'And while they may look like black boxes, deep down (sorry, I will stop the terrible puns) they are trying to accomplish the same thing as any other model — to make good predictions.',\n",
       " 'In this post, we will explore the ins and outs of a simple neural network.',\n",
       " 'And by the end, hopefully you (and I) will have gained a deeper and more intuitive understanding of how neural networks do what they do.',\n",
       " 'Let’s start with a really high level overview so we know what we are working with.',\n",
       " 'Neural networks are multi-layer networks of neurons (the blue and magenta nodes in the chart below) that we use to classify things, make predictions, etc.',\n",
       " 'Below is the diagram of a simple neural network with five inputs, 5 outputs, and two hidden layers of neurons.',\n",
       " 'Starting from the left, we have:\\n\\nThe input layer of our model in orange.',\n",
       " 'Our first hidden layer of neurons in blue.',\n",
       " 'Our second hidden layer of neurons in magenta.',\n",
       " 'The output layer (aka the prediction) of our model in green.',\n",
       " 'The arrows that connect the dots shows how all the neurons are interconnected and how data travels from the input layer all the way through to the output layer.',\n",
       " 'Later we will calculate step by step each output value.',\n",
       " 'We will also watch how the neural network learns from its mistake using a process known as backpropagation.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = tokenize.sent_tokenize(para)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: \n",
      "\n",
      "Top 10 most similar sentences in corpus:\n",
      "Let’s start with a really high level overview so we know what we are working with. (Score: 0.1488)\n",
      "And while they may look like black boxes, deep down (sorry, I will stop the terrible puns) they are trying to accomplish the same thing as any other model — to make good predictions. (Score: 0.1299)\n",
      "Our second hidden layer of neurons in magenta. (Score: 0.1161)\n",
      "Starting from the left, we have:\n",
      "\n",
      "The input layer of our model in orange. (Score: 0.1094)\n",
      "Our first hidden layer of neurons in blue. (Score: 0.1005)\n",
      "Neural networks are the workhorses of deep learning. (Score: 0.0761)\n",
      "In this post, we will explore the ins and outs of a simple neural network. (Score: 0.0651)\n",
      "We will also watch how the neural network learns from its mistake using a process known as backpropagation. (Score: 0.0607)\n",
      "And by the end, hopefully you (and I) will have gained a deeper and more intuitive understanding of how neural networks do what they do. (Score: 0.0589)\n",
      "That is a deep question (pardon the pun). (Score: 0.0581)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "corpus_embeddings = embedder.encode(sents, convert_to_tensor=True)\n",
    "\n",
    "# Query sentences:\n",
    "queries = ['']\n",
    "\n",
    "\n",
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "top_k = min(10, len(sents))\n",
    "for query in queries:\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "    print(\"\\n\\n======================\\n\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop 10 most similar sentences in corpus:\")\n",
    "\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        print(sents[idx], \"(Score: {:.4f})\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cdj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixed types\n",
      "0.18359439311764025 1\n",
      "[mixed types]\n",
      "systems\n",
      "0.1784796193107821 3\n",
      "[systems, systems, systems]\n",
      "minimal generating sets\n",
      "0.15037838042245094 1\n",
      "[minimal generating sets]\n",
      "nonstrict inequations\n",
      "0.14740065982407313 1\n",
      "[nonstrict inequations]\n",
      "strict inequations\n",
      "0.13946027725597837 1\n",
      "[strict inequations]\n",
      "linear Diophantine equations\n",
      "0.1195023546245721 1\n",
      "[linear Diophantine equations]\n",
      "natural numbers\n",
      "0.11450088293222845 1\n",
      "[natural numbers]\n",
      "solutions\n",
      "0.10780718173686318 3\n",
      "[solutions, solutions, solutions]\n",
      "linear constraints\n",
      "0.10529828014583348 1\n",
      "[linear constraints]\n",
      "all the considered types systems\n",
      "0.1036960590708142 1\n",
      "[all the considered types systems]\n",
      "a minimal supporting set\n",
      "0.08812713074893187 1\n",
      "[a minimal supporting set]\n",
      "a system\n",
      "0.08243620500315359 1\n",
      "[a system]\n",
      "a minimal set\n",
      "0.07944607954086784 1\n",
      "[a minimal set]\n",
      "algorithms\n",
      "0.0763527926213032 1\n",
      "[algorithms]\n",
      "all types\n",
      "0.07593126037016427 1\n",
      "[all types]\n",
      "Diophantine\n",
      "0.07309361902551355 1\n",
      "[Diophantine]\n",
      "construction\n",
      "0.0702090100898443 1\n",
      "[construction]\n",
      "Upper bounds\n",
      "0.060225391238828516 1\n",
      "[Upper bounds]\n",
      "the set\n",
      "0.05800111772673988 1\n",
      "[the set]\n",
      "components\n",
      "0.054251394765316464 1\n",
      "[components]\n",
      "Compatibility\n",
      "0.04516904342912139 1\n",
      "[Compatibility]\n",
      "compatibility\n",
      "0.04516904342912139 1\n",
      "[compatibility]\n",
      "the corresponding algorithms\n",
      "0.04435648606848154 1\n",
      "[the corresponding algorithms]\n",
      "Criteria\n",
      "0.042273783712246285 1\n",
      "[Criteria]\n",
      "These criteria\n",
      "0.01952542432474353 1\n",
      "[These criteria]\n"
     ]
    }
   ],
   "source": [
    "#### Pytextrank to find important phrases\n",
    "\n",
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "# example text\n",
    "text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\"\n",
    "\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# examine the top-ranked phrases in the document\n",
    "for phrase in doc._.phrases:\n",
    "    print(phrase.text)\n",
    "    print(phrase.rank, phrase.count)\n",
    "    print(phrase.chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf approach which helps to summarize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Harry Potter is a series of seven fantasy novels written by British author J. K. Rowling. The novels chronicle the lives of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry. The main story arc concerns Harry's struggle against Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard governing body known as the Ministry of Magic and subjugate all wizards and Muggles (non-magical people).\n",
    "\n",
    "The series was originally published in English by Bloomsbury in the United Kingdom and Scholastic Press in the United States. All versions around the world are printed by Grafica Veneta in Italy.[1] A series of many genres, including fantasy, drama, coming of age, and the British school story (which includes elements of mystery, thriller, adventure, horror, and romance), the world of Harry Potter explores numerous themes and includes many cultural meanings and references.[2] According to Rowling, the main theme is death.[3] Other major themes in the series include prejudice, corruption, and madness.[4]\n",
    "\n",
    "Since the release of the first novel, Harry Potter and the Philosopher's Stone, on 26 June 1997, the books have found immense popularity, positive reviews, and commercial success worldwide. They have attracted a wide adult audience as well as younger readers and are often considered cornerstones of modern young adult literature.[5] As of February 2018, the books have sold more than 500 million copies worldwide, making them the best-selling book series in history, and have been translated into more than eighty languages.[6] The last four books consecutively set records as the fastest-selling books in history, with the final instalment selling roughly 2.7 million copies in the United Kingdom and 8.3 million copies in the United States within twenty-four hours of its release.\n",
    "\n",
    "The original seven books were adapted into an eight-part namesake film series by Warner Bros. Pictures. In 2016, the total value of the Harry Potter franchise was estimated at $25 billion,[7] making Harry Potter one of the highest-grossing media franchises of all time. Harry Potter and the Cursed Child is a play based on a story co-written by Rowling.\n",
    "\n",
    "The success of the books and films has allowed the Harry Potter franchise to expand with numerous derivative works, a travelling exhibition that premiered in Chicago in 2009, a studio tour in London that opened in 2012, a digital platform on which J. K. Rowling updates the series with new information and insight, and a pentalogy of spin-off films premiering in November 2016 with Fantastic Beasts and Where to Find Them, among many other developments. Themed attractions, collectively known as The Wizarding World of Harry Potter, have been built at several Universal Parks & Resorts amusement parks around the world.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_frequency_matrix(sentences):\n",
    "    frequency_matrix = {}\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    lemma = WordNetLemmatizer()\n",
    "\n",
    "    for sent in sentences:\n",
    "        freq_table = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = lemma.lemmatize(word)\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        frequency_matrix[sent[:15]] = freq_table\n",
    "\n",
    "    return frequency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_tf_matrix(freq_matrix):\n",
    "    tf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        tf_table = {}\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, count in f_table.items():\n",
    "            tf_table[word] = count / count_words_in_sentence\n",
    "\n",
    "        tf_matrix[sent] = tf_table\n",
    "\n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_documents_per_words(freq_matrix):\n",
    "    word_per_doc_table = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        for word, count in f_table.items():\n",
    "            if word in word_per_doc_table:\n",
    "                word_per_doc_table[word] += 1\n",
    "            else:\n",
    "                word_per_doc_table[word] = 1\n",
    "\n",
    "    return word_per_doc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
    "    idf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        idf_table = {}\n",
    "\n",
    "        for word in f_table.keys():\n",
    "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
    "\n",
    "        idf_matrix[sent] = idf_table\n",
    "\n",
    "    return idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "\n",
    "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "\n",
    "        tf_idf_table = {}\n",
    "\n",
    "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
    "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "\n",
    "        tf_idf_matrix[sent1] = tf_idf_table\n",
    "\n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _score_sentences(tf_idf_matrix) -> dict:\n",
    "    \"\"\"\n",
    "    score a sentence by its word's TF\n",
    "    Basic algorithm: adding the TF frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    sentenceValue = {}\n",
    "\n",
    "    for sent, f_table in tf_idf_matrix.items():\n",
    "        total_score_per_sentence = 0\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, score in f_table.items():\n",
    "            total_score_per_sentence += score\n",
    "\n",
    "        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n",
    "\n",
    "    return sentenceValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_average_score(sentenceValue) -> int:\n",
    "    \"\"\"\n",
    "    Find the average score from the sentence value dictionary\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    sumValues = 0\n",
    "    for entry in sentenceValue:\n",
    "        sumValues += sentenceValue[entry]\n",
    "\n",
    "    # Average value of a sentence from original summary_text\n",
    "    average = (sumValues / len(sentenceValue))\n",
    "\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_summary(sentences, sentenceValue, threshold):\n",
    "    sentence_count = 0\n",
    "    summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence[:15] in sentenceValue and sentenceValue[sentence[:15]] >= (threshold):\n",
    "            summary += \" \" + sentence\n",
    "            sentence_count += 1\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mihir\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The series was originally published in English by Bloomsbury in the United Kingdom and Scholastic Press in the United States. All versions around the world are printed by Grafica Veneta in Italy. [2] According to Rowling, the main theme is death. [3] Other major themes in the series include prejudice, corruption, and madness. They have attracted a wide adult audience as well as younger readers and are often considered cornerstones of modern young adult literature. The original seven books were adapted into an eight-part namesake film series by Warner Bros. Pictures. Harry Potter and the Cursed Child is a play based on a story co-written by Rowling.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords    \n",
    "    \n",
    "'''\n",
    "We already have a sentence tokenizer, so we just need \n",
    "to run the sent_tokenize() method to create the array of sentences.\n",
    "'''\n",
    "# 1 Sentence Tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "total_documents = len(sentences)\n",
    "#print(sentences)\n",
    "\n",
    "# 2 Create the Frequency matrix of the words in each sentence.\n",
    "freq_matrix = _create_frequency_matrix(sentences)\n",
    "#print(freq_matrix)\n",
    "\n",
    "'''\n",
    "Term frequency (TF) is how often a word appears in a document, divided by how many words are there in a document.\n",
    "'''\n",
    "# 3 Calculate TermFrequency and generate a matrix\n",
    "tf_matrix = _create_tf_matrix(freq_matrix)\n",
    "#print(tf_matrix)\n",
    "\n",
    "# 4 creating table for documents per words\n",
    "count_doc_per_words = _create_documents_per_words(freq_matrix)\n",
    "#print(count_doc_per_words)\n",
    "\n",
    "'''\n",
    "Inverse document frequency (IDF) is how unique or rare a word is.\n",
    "'''\n",
    "# 5 Calculate IDF and generate a matrix\n",
    "idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
    "#print(idf_matrix)\n",
    "\n",
    "# 6 Calculate TF-IDF and generate a matrix\n",
    "tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
    "#print(tf_idf_matrix)\n",
    "\n",
    "# 7 Important Algorithm: score the sentences\n",
    "sentence_scores = _score_sentences(tf_idf_matrix)\n",
    "#print(sentence_scores)\n",
    "\n",
    "# 8 Find the threshold\n",
    "threshold = _find_average_score(sentence_scores)\n",
    "#print(threshold)\n",
    "\n",
    "# 9 Important Algorithm: Generate the summary\n",
    "summary = _generate_summary(sentences, sentence_scores, threshold)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value: adult\n",
      "[('adult', 0.1757784173397534), ('version', 0.15380611517228424), ('printed', 0.15380611517228424), ('grafica', 0.15380611517228424), ('veneta', 0.15380611517228424)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "dc = {}\n",
    "for i in tf_idf_matrix:\n",
    "    for j in tf_idf_matrix[i]:\n",
    "        dc[j] = tf_idf_matrix[i][j]\n",
    "fin_max = max(dc, key=dc.get)\n",
    "print(\"Maximum value:\",fin_max)\n",
    "print(Counter(dc).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting summa\n",
      "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
      "Requirement already satisfied: scipy>=0.19 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from summa) (1.8.1)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.17.3 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from scipy>=0.19->summa) (1.21.5)\n",
      "Building wheels for collected packages: summa\n",
      "  Building wheel for summa (setup.py): started\n",
      "  Building wheel for summa (setup.py): finished with status 'done'\n",
      "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54411 sha256=f430a7409905bd23344561117acb8d95636ba7e16289a1543990bcd58af87b3b\n",
      "  Stored in directory: c:\\users\\mihir\\appdata\\local\\pip\\cache\\wheels\\ed\\2c\\5f\\a0ccc5955d44d2cea78729f4425e73f818d2629517f7af0f8b\n",
      "Successfully built summa\n",
      "Installing collected packages: summa\n",
      "Successfully installed summa-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('adult', 0.3380449104576557), ('theme', 0.21790166957939905), ('themes', 0.21790166957939905), ('united', 0.21790166957939883), ('originally', 0.19383195948940687), ('original seven', 0.19383195948940676), ('bros', 0.1836023327011601), ('harry', 0.18360233270115994), ('film', 0.17862367456062173)]\n"
     ]
    }
   ],
   "source": [
    "TR_keywords = keywords.keywords(summary, scores=True)\n",
    "print(TR_keywords[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keybert\n",
      "  Downloading keybert-0.5.1.tar.gz (19 kB)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from keybert) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from keybert) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from keybert) (1.21.5)\n",
      "Collecting rich>=10.4.0\n",
      "  Downloading rich-12.5.1-py3-none-any.whl (235 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (2.11.2)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.8.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (3.7)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.64.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (1.12.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.1.96)\n",
      "Requirement already satisfied: torchvision in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.13.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (4.1.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.0.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.3.15)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.12.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers>=0.3.8->keybert) (0.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2021.10.8)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (9.0.1)\n",
      "Building wheels for collected packages: keybert\n",
      "  Building wheel for keybert (setup.py): started\n",
      "  Building wheel for keybert (setup.py): finished with status 'done'\n",
      "  Created wheel for keybert: filename=keybert-0.5.1-py3-none-any.whl size=21332 sha256=7f4da0a0827a80cbf122ed41c197f6d28d1deb5a869f7f4cff100b95bf72ddd3\n",
      "  Stored in directory: c:\\users\\mihir\\appdata\\local\\pip\\cache\\wheels\\94\\18\\2a\\f26bbcd25924aab452bb4bcc2345a55c07160823d196a264c7\n",
      "Successfully built keybert\n",
      "Installing collected packages: commonmark, rich, keybert\n",
      "Successfully installed commonmark-0.9.1 keybert-0.5.1 rich-12.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7ea1b1f14c4ce598419fdc21c00add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f875f10a77489ea5ca7ddc6bb36c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c35a5dbb75c4d2faa96768d4240ee60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6d189613984d539f9711bcabb7d269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddfdbf7b70146b995c2bb5e640a0bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31498ee7bdf24a9c966cd8631cee65fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce9b43807ed4e4e80883a5d70321e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5537f88b3a94335a8e70e50c5be3f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860378015f3343928ba832f89ec1f5ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05813adfcce49ffa529cfb8c10b87ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58b5066bc0541b6ba6be0069be3b2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d26c7793a9b477f88db8bb7ab14cbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342da009543047c898a45d5df6c036f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73c1aff70df48feab65f046550e1c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kw_model = KeyBERT(model='all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potter cursed child', 'harry potter cursed', 'rowling main theme', 'potter cursed', 'story written rowling', 'cursed child play', 'harry potter', 'cursed child', 'original seven books', 'written rowling']\n"
     ]
    }
   ],
   "source": [
    "keywords = kw_model.extract_keywords(summary, \n",
    "\n",
    "                                     keyphrase_ngram_range=(1, 3), \n",
    "\n",
    "                                     stop_words='english', \n",
    "\n",
    "                                     highlight=False,\n",
    "\n",
    "                                     top_n=10)\n",
    "\n",
    "keywords_list= list(dict(keywords).keys())\n",
    "\n",
    "print(keywords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "47c626452ef4ef3e74376d35c302fcf9bdc1b9327d6e04736eb914a557504e89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
